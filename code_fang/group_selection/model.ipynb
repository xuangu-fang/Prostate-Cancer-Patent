{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('pt': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cb03ba31accc2f25e369d1aa3152af3569125a23627c23321dfacf015e0ff229"
   }
  },
  "interpreter": {
   "hash": "cb03ba31accc2f25e369d1aa3152af3569125a23627c23321dfacf015e0ff229"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy\n",
    "# import xlrd \n",
    "import sklearn\n",
    "\n",
    "from Gibbs_model import Gibbs_sampling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import multivariate_normal\n",
    "from utils import baseline_lr,baseline_esnet,baseline_lrCV,baseline_esnetCV,baseline_justmean\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import binom \n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import Ridge\n",
    "from tqdm import trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loading \n",
    "np.random.seed(123)\n",
    "\n",
    "data_table = pd.read_csv('../data/processed/all_feature_p2_lip_specie.csv')\n",
    "# data_table = pd.read_csv('../data/processed/all_feature_p1_lip_specie.csv')\n",
    "target = 'gap_death_consent'\n",
    "\n",
    "\n",
    "# normalization\n",
    "\n",
    "# min-max\n",
    "# df = data_table[target]\n",
    "# data_table[target] = (df-df.min())/(df.max()-df.min())\n",
    "\n",
    "# log\n",
    "df = data_table[target]\n",
    "data_table[target] =np.log(df + 1.0)\n",
    "# data_table[target] = (data_table[target]-data_table[target].mean())/data_table[target].std()\n",
    "\n",
    "# check nan\n",
    "data_table[target].isnull().values.any()\n",
    "\n",
    "data_table.fillna(data_table.mean(), inplace=True) # fill nan with column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_feature = data_table.iloc[:,1:-368]#for p2\n",
    "gene_feature = data_table.iloc[:,1:775]#for p1\n",
    "\n",
    "sur_time = data_table[target].values\n",
    "Y = sur_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     lip_Sph(d16:1)_27  lip_Sph(d18:1)_27  lip_Sph(d18:2)_27  \\\n",
       "0             1.508517          -1.142981          -0.370896   \n",
       "1             0.662510          -0.169350           0.276919   \n",
       "2             0.382097           0.034175          -0.873632   \n",
       "3            -0.505375          -0.856168          -0.309318   \n",
       "4            -0.973029          -1.221808           0.084801   \n",
       "..                 ...                ...                ...   \n",
       "139           0.836448           0.078052          -0.680276   \n",
       "140           0.840385           0.370737          -0.795050   \n",
       "141           0.958798          -0.450028           0.631433   \n",
       "142          -0.486482          -0.715181          -0.765942   \n",
       "143           0.517969           0.222055           1.282967   \n",
       "\n",
       "     lip_S1P(d16:1)_6  lip_S1P(d18:0)_6  lip_S1P(d18:1)_6  lip_S1P(d18:2)_6  \\\n",
       "0            1.161898          1.393186          2.439157          3.142364   \n",
       "1           -0.420598          0.890299          1.172982          0.270768   \n",
       "2            0.263159          0.442293         -0.246913         -0.094627   \n",
       "3           -0.876899          0.129396         -0.332582         -0.197593   \n",
       "4            1.401033          0.822457          0.869329          0.284968   \n",
       "..                ...               ...               ...               ...   \n",
       "139         -0.222538         -1.176785         -1.582970         -1.824283   \n",
       "140         -0.544087         -0.086341         -0.931198         -1.267266   \n",
       "141          1.413756         -0.018409         -0.517109          0.988292   \n",
       "142         -1.725063         -1.572528         -1.536034         -1.350552   \n",
       "143         -0.977544         -0.668532         -0.798935         -1.308465   \n",
       "\n",
       "     lip_Cer(d18:0/16:0)_3  lip_Cer(d18:0/18:0)_3  lip_Cer(d18:0/20:0)_3  ...  \\\n",
       "0                 2.807684               1.906719               1.964852  ...   \n",
       "1                 1.235381              -0.120123               0.356863  ...   \n",
       "2                 0.488499              -1.068774              -0.575590  ...   \n",
       "3                 1.159958               1.085276               0.743728  ...   \n",
       "4                 0.232719               0.030465              -0.985736  ...   \n",
       "..                     ...                    ...                    ...  ...   \n",
       "139               0.018785               1.688315               0.036113  ...   \n",
       "140              -0.992808               0.144297              -0.843535  ...   \n",
       "141              -1.232623               0.418946              -0.962435  ...   \n",
       "142              -1.357425              -1.695333              -0.907401  ...   \n",
       "143               0.653114               0.069228               0.664920  ...   \n",
       "\n",
       "     lip_TG(O-54:4) [NL-18:2]_13  lip_Ubiquinone_24  lip_CE(18:2) [+OH]_15  \\\n",
       "0                       0.036007           0.359336              -0.383320   \n",
       "1                      -0.656573          -0.557216               4.805999   \n",
       "2                       2.257551          -0.525753              -0.165085   \n",
       "3                       1.085764           0.103507              -0.345465   \n",
       "4                      -1.369647          -1.081999               5.153219   \n",
       "..                           ...                ...                    ...   \n",
       "139                    -0.919553          -0.499796               0.454661   \n",
       "140                    -0.504697          -0.009101               0.544398   \n",
       "141                    -0.217015           0.367528               0.289700   \n",
       "142                    -0.470357          -0.925047               0.599696   \n",
       "143                     1.043798          -0.149132               0.092477   \n",
       "\n",
       "     lip_CE(20:4) [+OH]_15  lip_CE(22:6) [+OH]_15  lip_LPC(18:2) [+OH]_15  \\\n",
       "0                -0.018562              -0.205400               -0.186943   \n",
       "1                 2.009266               2.187153               11.290922   \n",
       "2                -0.231418              -0.152552               -0.118845   \n",
       "3                -0.515169              -0.190832               -0.202784   \n",
       "4                 7.136091               8.406937                4.119153   \n",
       "..                     ...                    ...                     ...   \n",
       "139               0.274775              -0.177827               -0.122108   \n",
       "140              -0.174427              -0.209744               -0.171430   \n",
       "141               0.035265              -0.179299               -0.172936   \n",
       "142              -0.066377              -0.217330               -0.155953   \n",
       "143               0.002583              -0.135165               -0.168080   \n",
       "\n",
       "     lip_LPC(20:4) [+OH]_15  lip_LPC(22:6) [+OH]_15  lip_PC(34:2) [+OH]_15  \\\n",
       "0                 -0.090134               -0.100703              -0.293183   \n",
       "1                  7.801393                8.273906              10.974755   \n",
       "2                 -0.172070               -0.142195              -0.062192   \n",
       "3                 -0.200526               -0.244971              -0.218103   \n",
       "4                  6.956574                8.631769               6.050933   \n",
       "..                      ...                     ...                    ...   \n",
       "139               -0.138097               -0.105071              -0.128006   \n",
       "140               -0.183107               -0.169494              -0.137458   \n",
       "141               -0.163742               -0.136950              -0.152169   \n",
       "142               -0.172993               -0.199861              -0.191259   \n",
       "143               -0.170067               -0.114229               0.082395   \n",
       "\n",
       "     lip_PC(36:4) [+OH]_15  \n",
       "0                 1.595553  \n",
       "1                 2.317701  \n",
       "2                 0.139186  \n",
       "3                -1.082805  \n",
       "4                 3.055967  \n",
       "..                     ...  \n",
       "139              -0.260284  \n",
       "140              -0.804552  \n",
       "141              -0.558493  \n",
       "142              -0.745833  \n",
       "143               0.530311  \n",
       "\n",
       "[144 rows x 774 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lip_Sph(d16:1)_27</th>\n      <th>lip_Sph(d18:1)_27</th>\n      <th>lip_Sph(d18:2)_27</th>\n      <th>lip_S1P(d16:1)_6</th>\n      <th>lip_S1P(d18:0)_6</th>\n      <th>lip_S1P(d18:1)_6</th>\n      <th>lip_S1P(d18:2)_6</th>\n      <th>lip_Cer(d18:0/16:0)_3</th>\n      <th>lip_Cer(d18:0/18:0)_3</th>\n      <th>lip_Cer(d18:0/20:0)_3</th>\n      <th>...</th>\n      <th>lip_TG(O-54:4) [NL-18:2]_13</th>\n      <th>lip_Ubiquinone_24</th>\n      <th>lip_CE(18:2) [+OH]_15</th>\n      <th>lip_CE(20:4) [+OH]_15</th>\n      <th>lip_CE(22:6) [+OH]_15</th>\n      <th>lip_LPC(18:2) [+OH]_15</th>\n      <th>lip_LPC(20:4) [+OH]_15</th>\n      <th>lip_LPC(22:6) [+OH]_15</th>\n      <th>lip_PC(34:2) [+OH]_15</th>\n      <th>lip_PC(36:4) [+OH]_15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.508517</td>\n      <td>-1.142981</td>\n      <td>-0.370896</td>\n      <td>1.161898</td>\n      <td>1.393186</td>\n      <td>2.439157</td>\n      <td>3.142364</td>\n      <td>2.807684</td>\n      <td>1.906719</td>\n      <td>1.964852</td>\n      <td>...</td>\n      <td>0.036007</td>\n      <td>0.359336</td>\n      <td>-0.383320</td>\n      <td>-0.018562</td>\n      <td>-0.205400</td>\n      <td>-0.186943</td>\n      <td>-0.090134</td>\n      <td>-0.100703</td>\n      <td>-0.293183</td>\n      <td>1.595553</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.662510</td>\n      <td>-0.169350</td>\n      <td>0.276919</td>\n      <td>-0.420598</td>\n      <td>0.890299</td>\n      <td>1.172982</td>\n      <td>0.270768</td>\n      <td>1.235381</td>\n      <td>-0.120123</td>\n      <td>0.356863</td>\n      <td>...</td>\n      <td>-0.656573</td>\n      <td>-0.557216</td>\n      <td>4.805999</td>\n      <td>2.009266</td>\n      <td>2.187153</td>\n      <td>11.290922</td>\n      <td>7.801393</td>\n      <td>8.273906</td>\n      <td>10.974755</td>\n      <td>2.317701</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.382097</td>\n      <td>0.034175</td>\n      <td>-0.873632</td>\n      <td>0.263159</td>\n      <td>0.442293</td>\n      <td>-0.246913</td>\n      <td>-0.094627</td>\n      <td>0.488499</td>\n      <td>-1.068774</td>\n      <td>-0.575590</td>\n      <td>...</td>\n      <td>2.257551</td>\n      <td>-0.525753</td>\n      <td>-0.165085</td>\n      <td>-0.231418</td>\n      <td>-0.152552</td>\n      <td>-0.118845</td>\n      <td>-0.172070</td>\n      <td>-0.142195</td>\n      <td>-0.062192</td>\n      <td>0.139186</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.505375</td>\n      <td>-0.856168</td>\n      <td>-0.309318</td>\n      <td>-0.876899</td>\n      <td>0.129396</td>\n      <td>-0.332582</td>\n      <td>-0.197593</td>\n      <td>1.159958</td>\n      <td>1.085276</td>\n      <td>0.743728</td>\n      <td>...</td>\n      <td>1.085764</td>\n      <td>0.103507</td>\n      <td>-0.345465</td>\n      <td>-0.515169</td>\n      <td>-0.190832</td>\n      <td>-0.202784</td>\n      <td>-0.200526</td>\n      <td>-0.244971</td>\n      <td>-0.218103</td>\n      <td>-1.082805</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.973029</td>\n      <td>-1.221808</td>\n      <td>0.084801</td>\n      <td>1.401033</td>\n      <td>0.822457</td>\n      <td>0.869329</td>\n      <td>0.284968</td>\n      <td>0.232719</td>\n      <td>0.030465</td>\n      <td>-0.985736</td>\n      <td>...</td>\n      <td>-1.369647</td>\n      <td>-1.081999</td>\n      <td>5.153219</td>\n      <td>7.136091</td>\n      <td>8.406937</td>\n      <td>4.119153</td>\n      <td>6.956574</td>\n      <td>8.631769</td>\n      <td>6.050933</td>\n      <td>3.055967</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>0.836448</td>\n      <td>0.078052</td>\n      <td>-0.680276</td>\n      <td>-0.222538</td>\n      <td>-1.176785</td>\n      <td>-1.582970</td>\n      <td>-1.824283</td>\n      <td>0.018785</td>\n      <td>1.688315</td>\n      <td>0.036113</td>\n      <td>...</td>\n      <td>-0.919553</td>\n      <td>-0.499796</td>\n      <td>0.454661</td>\n      <td>0.274775</td>\n      <td>-0.177827</td>\n      <td>-0.122108</td>\n      <td>-0.138097</td>\n      <td>-0.105071</td>\n      <td>-0.128006</td>\n      <td>-0.260284</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>0.840385</td>\n      <td>0.370737</td>\n      <td>-0.795050</td>\n      <td>-0.544087</td>\n      <td>-0.086341</td>\n      <td>-0.931198</td>\n      <td>-1.267266</td>\n      <td>-0.992808</td>\n      <td>0.144297</td>\n      <td>-0.843535</td>\n      <td>...</td>\n      <td>-0.504697</td>\n      <td>-0.009101</td>\n      <td>0.544398</td>\n      <td>-0.174427</td>\n      <td>-0.209744</td>\n      <td>-0.171430</td>\n      <td>-0.183107</td>\n      <td>-0.169494</td>\n      <td>-0.137458</td>\n      <td>-0.804552</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>0.958798</td>\n      <td>-0.450028</td>\n      <td>0.631433</td>\n      <td>1.413756</td>\n      <td>-0.018409</td>\n      <td>-0.517109</td>\n      <td>0.988292</td>\n      <td>-1.232623</td>\n      <td>0.418946</td>\n      <td>-0.962435</td>\n      <td>...</td>\n      <td>-0.217015</td>\n      <td>0.367528</td>\n      <td>0.289700</td>\n      <td>0.035265</td>\n      <td>-0.179299</td>\n      <td>-0.172936</td>\n      <td>-0.163742</td>\n      <td>-0.136950</td>\n      <td>-0.152169</td>\n      <td>-0.558493</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>-0.486482</td>\n      <td>-0.715181</td>\n      <td>-0.765942</td>\n      <td>-1.725063</td>\n      <td>-1.572528</td>\n      <td>-1.536034</td>\n      <td>-1.350552</td>\n      <td>-1.357425</td>\n      <td>-1.695333</td>\n      <td>-0.907401</td>\n      <td>...</td>\n      <td>-0.470357</td>\n      <td>-0.925047</td>\n      <td>0.599696</td>\n      <td>-0.066377</td>\n      <td>-0.217330</td>\n      <td>-0.155953</td>\n      <td>-0.172993</td>\n      <td>-0.199861</td>\n      <td>-0.191259</td>\n      <td>-0.745833</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>0.517969</td>\n      <td>0.222055</td>\n      <td>1.282967</td>\n      <td>-0.977544</td>\n      <td>-0.668532</td>\n      <td>-0.798935</td>\n      <td>-1.308465</td>\n      <td>0.653114</td>\n      <td>0.069228</td>\n      <td>0.664920</td>\n      <td>...</td>\n      <td>1.043798</td>\n      <td>-0.149132</td>\n      <td>0.092477</td>\n      <td>0.002583</td>\n      <td>-0.135165</td>\n      <td>-0.168080</td>\n      <td>-0.170067</td>\n      <td>-0.114229</td>\n      <td>0.082395</td>\n      <td>0.530311</td>\n    </tr>\n  </tbody>\n</table>\n<p>144 rows × 774 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "gene_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(gene_feature.columns)\n",
    "K=41 # group number, from data process notebook\n",
    "group_ind_dict = {}\n",
    "group_ind = []\n",
    "group_ind_concat = []\n",
    "for i in range(K):\n",
    "    group_ind_dict[i] = []\n",
    "\n",
    "for name in feature_names:\n",
    "    id = int(name.split('_')[-1])\n",
    "    group_ind_dict[id].append(name)\n",
    "\n",
    "for i in range(K):\n",
    "    group_ind.append(group_ind_dict[i])\n",
    "    group_ind_concat = group_ind_concat + group_ind_dict[i]\n",
    "\n",
    "# group_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(144, 775)\n"
     ]
    }
   ],
   "source": [
    "# re-arrange the features of X based on the group split order\n",
    "X_new = gene_feature[group_ind_concat].values\n",
    "\n",
    "N_sample,_ = X_new.shape\n",
    "# add all-one column at the last \n",
    "bias_col = np.ones(N_sample).reshape((N_sample,1))\n",
    "X_new = np.concatenate((X_new,bias_col),axis=1)\n",
    "\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init hyper-parameters\n",
    "alpha = 0.7\n",
    "beta = 0.7\n",
    "r0 = 1e-3\n",
    "r1 = 1.0\n",
    "a0 = 1.0\n",
    "b0 = 1.0\n",
    "JITTER = 1e-3\n",
    "\n",
    "INTERVAL = 100\n",
    "VALITA_INTERVAL = 200\n",
    "BURNING = 3000\n",
    "MAX_NUMBER = 5000\n",
    "\n",
    "hyper_paras = {'INTERVAL':INTERVAL, 'BURNING':BURNING,'MAX_NUMBER':MAX_NUMBER,'VALITA_INTERVAL':VALITA_INTERVAL,\n",
    "'alpha':alpha, 'beta':beta,'r0':r0,'r1':r1,'JITTER':JITTER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.012546227404933663, tolerance: 0.006541115990702146\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010116891867582911, tolerance: 0.005951692591439892\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0072613163792305535, tolerance: 0.005951692591439892\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01431715627117125, tolerance: 0.005951692591439892\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.006961491770257178, tolerance: 0.005951692591439892\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0076187375852816985, tolerance: 0.005951692591439892\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.008654955685202748, tolerance: 0.005951692591439892\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.005990285669531081, tolerance: 0.005951692591439892\n",
      "  positive)\n",
      "with elanetCV, rmse is 1004.17970\n",
      "with just-mean, rmse is 982.70664\n",
      "with lrCV, rmse is 2129.18554\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.011524876512083448, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.012923992159703523, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015142920090899459, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020789259206572375, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0143612208002879, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.023195275477924326, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.019581648685711883, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.014455620830011284, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.030078095277123573, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.016681957406100345, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03369216251693663, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.052970752436173996, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.014640158100728051, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.028168261934290673, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02371055570984648, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010904880815009399, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010152334288648035, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.014498445625198642, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.025725642203351384, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.013548741579711532, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03299335741549925, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01962304989432262, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.048827805847603045, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015970757165194804, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02746447300657895, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01132963575941659, tolerance: 0.006267952908107518\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.013403174252891503, tolerance: 0.006267952908107518\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.006716331963596112, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010733913149333918, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.008076809258425133, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010192116025784292, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.008873781931270308, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.009643626705384545, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.008343633649878446, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.009227668203116934, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.007816900311771513, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00789748361349718, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00977737755928021, tolerance: 0.006456506694592236\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.012449271969163345, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0114153860742503, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010661083333001198, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015031898174310054, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01244545990428847, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010545106110813052, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015967104221001227, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.013462871697425438, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.019292388671093397, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.019149298860615005, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01010534797400342, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.009110476326034678, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.008547979967965413, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.012033106189767662, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01013598200760979, tolerance: 0.0065836147955700575\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.008202350036716322, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.008024584463872442, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.011490441703228216, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010451280338815594, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.007157623203353447, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.006887834158221251, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.008923546583273767, tolerance: 0.006418539082653409\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.011061266502352396, tolerance: 0.006267952908107518\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.012871637990767404, tolerance: 0.006267952908107518\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010316398794002701, tolerance: 0.006267952908107518\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.011847284362677402, tolerance: 0.006267952908107518\n",
      "  positive)\n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.014557273197975007, tolerance: 0.006267952908107518\n",
      "  positive)\n",
      "with elanetCV, rmse is 823.38740\n",
      "with just-mean, rmse is 802.04755\n",
      "lr_rmse_mean: 8202.3142,lr_rmse_std: 21419.0925 \n",
      "esnet_rmse_mean: 1027.7957,esnet_rmse_std: 585.5447 \n",
      "just-mean_rmse_mean: 872.8602,mean_rmse_std: 76.9281 \n",
      "G:\\anaconda\\envs\\pt\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.007687991206277833, tolerance: 0.006267952908107518\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = 50\n",
    "lr_rmse = np.zeros(N)\n",
    "esnet_rmse = np.zeros(N)\n",
    "mean_rmse = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, Y.squeeze(),test_size=0.5)\n",
    "\n",
    "    y_mean_tr = y_train.mean()\n",
    "    y_std_tr = y_train.std()\n",
    "    y_nor_tr = (y_train-y_mean_tr)/y_std_tr\n",
    "    data_dict = {'X_tr':X_train, 'y_nor_tr':y_nor_tr, 'X_test':X_test, 'y_test':y_test,'y_mean_tr': y_mean_tr, 'y_std_tr':y_std_tr}    \n",
    "    w_lr,lr_rmse[i] = baseline_lrCV(data_dict)\n",
    "    w_es,esnet_rmse[i] = baseline_esnetCV(data_dict)\n",
    "    mean_rmse[i] = baseline_justmean(data_dict)\n",
    "\n",
    "print('lr_rmse_mean: %.4f,lr_rmse_std: %.4f '%(lr_rmse.mean(),lr_rmse.std() ) )\n",
    "print('esnet_rmse_mean: %.4f,esnet_rmse_std: %.4f '%(esnet_rmse.mean(),esnet_rmse.std() ) )\n",
    "print('just-mean_rmse_mean: %.4f,mean_rmse_std: %.4f '%(mean_rmse.mean(),mean_rmse.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "with just-mean, rmse is 1.38379\n"
     ]
    }
   ],
   "source": [
    "predict =  data_dict['y_mean_tr']\n",
    "rmse = np.sqrt(np.mean((predict-data_dict['y_test'].squeeze())**2))\n",
    "print('with just-mean, rmse is %.5f'%(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init parameters\n",
    "# z_array_init = np.random.binomial(size=K, n=1, p= alpha)\n",
    "# s_list_init = [np.random.binomial(size=len(item), n=1, p= beta) for item in group_ind]\n",
    "# b_init = np.random.normal(loc=0.0, scale=r1,size=None)\n",
    "# tau_init = np.random.gamma(shape=alpha, scale=1.0/beta, size=None)\n",
    "# W_init = []\n",
    "\n",
    "# for i in range(K):\n",
    "#     mask1 = 1-z_array_init[i] * s_list_init[i]\n",
    "#     mask2 = z_array_init[i] * s_list_init[i]\n",
    "#     spike = np.random.normal(loc=0.0, scale=r0,size=len(s_list_init[i]))\n",
    "#     slab = np.random.normal(loc=0.0, scale=r1,size=len(s_list_init[i]))\n",
    "#     W_group = spike * mask1 + slab * mask2\n",
    "\n",
    "#     W_init.append(W_group)\n",
    "\n",
    "# init_paras = {'z':z_array_init, 's':s_list_init, 'b':b_init, 'tau':tau_init, 'W':W_init,'a0':a0,'b0':b0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init parameters with lr_result\n",
    "def get_init_paras(w_lr):\n",
    "    z_array_init = np.ones(K) #np.random.binomial(size=K, n=1, p= alpha)\n",
    "    s_list_init = [np.random.binomial(size=len(item), n=1, p= beta) for item in group_ind]\n",
    "    b_init = w_lr[-1]#np.random.normal(loc=0.0, scale=r1,size=None)\n",
    "    tau_init = 1.0#np.random.gamma(shape=alpha, scale=1.0/beta, size=None)\n",
    "    W_init = []\n",
    "    offset=0\n",
    "    for i in range(K):\n",
    "        # mask1 = 1-z_array_init[i] * s_list_init[i]\n",
    "        # mask2 = z_array_init[i] * s_list_init[i]\n",
    "        # spike = np.random.normal(loc=0.0, scale=r0,size=len(s_list_init[i]))\n",
    "        # slab = np.random.normal(loc=0.0, scale=r1,size=len(s_list_init[i]))\n",
    "        # W_group = spike * mask1 + slab * mask2\n",
    "\n",
    "        \n",
    "        group_len = len(s_list_init[i])\n",
    "        W_group= w_lr[offset:offset+group_len]\n",
    "        offset = offset + group_len\n",
    "        W_init.append(W_group)\n",
    "\n",
    "    init_paras = {'z':z_array_init, 's':s_list_init, 'b':b_init, 'tau':tau_init, 'W':W_init,'a0':a0,'b0':b0}\n",
    "    return init_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "get_init_paras(w_lr)['tau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 1/5000 [00:00<08:58,  9.28it/s]with lr, rmse is 1.23931\n",
      "with elanet, rmse is 0.90610\n",
      "\n",
      " running test-rmse = 1.31665\n",
      "running train-rmse = 0.72845\n",
      "\n",
      "  4%|▍         | 202/5000 [00:21<08:22,  9.55it/s]\n",
      " running test-rmse = 1.17275\n",
      "running train-rmse = 0.80951\n",
      "\n",
      "  8%|▊         | 402/5000 [00:43<09:06,  8.41it/s]\n",
      " running test-rmse = 1.22125\n",
      "running train-rmse = 0.83236\n",
      "\n",
      " 12%|█▏        | 602/5000 [01:05<07:55,  9.24it/s]\n",
      " running test-rmse = 1.64747\n",
      "running train-rmse = 0.78348\n",
      "\n",
      " 16%|█▌        | 802/5000 [01:26<08:27,  8.28it/s]\n",
      " running test-rmse = 1.38720\n",
      "running train-rmse = 1.02710\n",
      "\n",
      " 20%|██        | 1002/5000 [01:50<08:19,  8.00it/s]\n",
      " running test-rmse = 1.00437\n",
      "running train-rmse = 0.95954\n",
      "\n",
      " 24%|██▍       | 1202/5000 [02:14<06:40,  9.48it/s]\n",
      " running test-rmse = 1.16492\n",
      "running train-rmse = 0.91064\n",
      "\n",
      " 28%|██▊       | 1402/5000 [02:36<06:50,  8.76it/s]\n",
      " running test-rmse = 0.87052\n",
      "running train-rmse = 0.85004\n",
      "\n",
      " 32%|███▏      | 1602/5000 [02:58<06:05,  9.30it/s]\n",
      " running test-rmse = 0.84245\n",
      "running train-rmse = 0.91741\n",
      "\n",
      " 36%|███▌      | 1802/5000 [03:19<05:45,  9.27it/s]\n",
      " running test-rmse = 0.90206\n",
      "running train-rmse = 0.92266\n",
      "\n",
      " 40%|████      | 2002/5000 [03:41<05:19,  9.38it/s]\n",
      " running test-rmse = 0.98814\n",
      "running train-rmse = 0.90660\n",
      "\n",
      " 44%|████▍     | 2202/5000 [04:02<04:57,  9.39it/s]\n",
      " running test-rmse = 1.16272\n",
      "running train-rmse = 0.96328\n",
      "\n",
      " 48%|████▊     | 2402/5000 [04:25<04:48,  9.01it/s]\n",
      " running test-rmse = 1.26121\n",
      "running train-rmse = 0.80610\n",
      "\n",
      " 52%|█████▏    | 2602/5000 [04:47<04:30,  8.87it/s]\n",
      " running test-rmse = 1.53452\n",
      "running train-rmse = 0.82159\n",
      "\n",
      " 56%|█████▌    | 2803/5000 [05:09<03:42,  9.89it/s]\n",
      " running test-rmse = 1.23385\n",
      "running train-rmse = 0.97691\n",
      "\n",
      " 60%|██████    | 3002/5000 [05:30<03:23,  9.82it/s]\n",
      " running test-rmse = 1.03413\n",
      "running train-rmse = 1.02569\n",
      "\n",
      " 64%|██████▍   | 3202/5000 [05:51<03:11,  9.40it/s]\n",
      " running test-rmse = 1.02557\n",
      "running train-rmse = 0.96832\n",
      "\n",
      " 68%|██████▊   | 3402/5000 [06:13<02:51,  9.31it/s]\n",
      " running test-rmse = 0.74345\n",
      "running train-rmse = 1.05803\n",
      "\n",
      " 72%|███████▏  | 3602/5000 [06:34<02:25,  9.60it/s]\n",
      " running test-rmse = 1.20667\n",
      "running train-rmse = 0.97685\n",
      "\n",
      " 76%|███████▌  | 3802/5000 [06:55<02:08,  9.31it/s]\n",
      " running test-rmse = 0.96207\n",
      "running train-rmse = 0.88595\n",
      "\n",
      " 80%|████████  | 4002/5000 [07:16<01:46,  9.34it/s]\n",
      " running test-rmse = 1.05902\n",
      "running train-rmse = 0.89159\n",
      "\n",
      " 84%|████████▍ | 4202/5000 [07:37<01:25,  9.32it/s]\n",
      " running test-rmse = 1.15737\n",
      "running train-rmse = 0.81887\n",
      "\n",
      " 88%|████████▊ | 4402/5000 [07:58<01:03,  9.43it/s]\n",
      " running test-rmse = 1.33239\n",
      "running train-rmse = 0.85900\n",
      "\n",
      " 92%|█████████▏| 4602/5000 [08:20<00:41,  9.51it/s]\n",
      " running test-rmse = 0.93627\n",
      "running train-rmse = 1.04517\n",
      "\n",
      " 96%|█████████▌| 4802/5000 [08:41<00:20,  9.44it/s]\n",
      " running test-rmse = 1.03741\n",
      "running train-rmse = 0.82478\n",
      "\n",
      "100%|██████████| 5000/5000 [09:02<00:00,  9.21it/s]\n",
      "  0%|          | 1/5000 [00:00<08:48,  9.46it/s]\n",
      "\n",
      " final test rmse = 1.00660\n",
      "\n",
      "\n",
      " final test rmse = 1.00660\n",
      "with lr, rmse is 1.11109\n",
      "with elanet, rmse is 0.85917\n",
      "\n",
      " running test-rmse = 1.23159\n",
      "running train-rmse = 0.96251\n",
      "\n",
      "  4%|▍         | 202/5000 [00:21<08:22,  9.55it/s]\n",
      " running test-rmse = 0.92586\n",
      "running train-rmse = 0.90925\n",
      "\n",
      "  8%|▊         | 402/5000 [00:42<08:01,  9.55it/s]\n",
      " running test-rmse = 0.97608\n",
      "running train-rmse = 0.92703\n",
      "\n",
      " 12%|█▏        | 601/5000 [01:04<08:13,  8.91it/s]\n",
      " running test-rmse = 1.05132\n",
      "running train-rmse = 0.99606\n",
      "\n",
      " 16%|█▌        | 802/5000 [01:25<07:42,  9.08it/s]\n",
      " running test-rmse = 1.01570\n",
      "running train-rmse = 0.87263\n",
      "\n",
      " 20%|██        | 1002/5000 [01:48<07:28,  8.92it/s]\n",
      " running test-rmse = 1.09686\n",
      "running train-rmse = 0.99779\n",
      "\n",
      " 24%|██▍       | 1201/5000 [02:09<06:53,  9.20it/s]\n",
      " running test-rmse = 1.15394\n",
      "running train-rmse = 0.95517\n",
      "\n",
      " 28%|██▊       | 1402/5000 [02:31<06:34,  9.12it/s]\n",
      " running test-rmse = 4.76064\n",
      "running train-rmse = 1.00654\n",
      "\n",
      " 32%|███▏      | 1602/5000 [02:53<06:20,  8.93it/s]\n",
      " running test-rmse = 1.14333\n",
      "running train-rmse = 0.87168\n",
      "\n",
      " 36%|███▌      | 1802/5000 [03:15<05:44,  9.30it/s]\n",
      " running test-rmse = 0.93827\n",
      "running train-rmse = 0.94718\n",
      "\n",
      " 40%|████      | 2002/5000 [03:36<05:34,  8.96it/s]\n",
      " running test-rmse = 2.24101\n",
      "running train-rmse = 1.02614\n",
      "\n",
      " 44%|████▍     | 2203/5000 [03:58<04:44,  9.84it/s]\n",
      " running test-rmse = 1.64570\n",
      "running train-rmse = 0.98364\n",
      "\n",
      " 48%|████▊     | 2402/5000 [04:20<04:53,  8.84it/s]\n",
      " running test-rmse = 1.45680\n",
      "running train-rmse = 0.97114\n",
      "\n",
      " 52%|█████▏    | 2602/5000 [04:43<04:32,  8.81it/s]\n",
      " running test-rmse = 1.16949\n",
      "running train-rmse = 0.90380\n",
      "\n",
      " 56%|█████▌    | 2802/5000 [05:05<04:02,  9.05it/s]\n",
      " running test-rmse = 1.18474\n",
      "running train-rmse = 0.82648\n",
      "\n",
      " 60%|██████    | 3002/5000 [05:28<03:42,  8.99it/s]\n",
      " running test-rmse = 1.11620\n",
      "running train-rmse = 0.99887\n",
      "\n",
      " 64%|██████▍   | 3202/5000 [05:50<03:29,  8.58it/s]\n",
      " running test-rmse = 3.01069\n",
      "running train-rmse = 0.91682\n",
      "\n",
      " 68%|██████▊   | 3402/5000 [06:13<03:07,  8.54it/s]\n",
      " running test-rmse = 1.09757\n",
      "running train-rmse = 0.93818\n",
      "\n",
      " 72%|███████▏  | 3602/5000 [06:35<02:35,  8.99it/s]\n",
      " running test-rmse = 0.90537\n",
      "running train-rmse = 0.98121\n",
      "\n",
      " 76%|███████▌  | 3802/5000 [06:58<02:20,  8.54it/s]\n",
      " running test-rmse = 0.96298\n",
      "running train-rmse = 0.99297\n",
      "\n",
      " 80%|████████  | 4002/5000 [07:20<01:51,  8.96it/s]\n",
      " running test-rmse = 0.97551\n",
      "running train-rmse = 0.88955\n",
      "\n",
      " 84%|████████▍ | 4202/5000 [07:41<01:24,  9.45it/s]\n",
      " running test-rmse = 1.11276\n",
      "running train-rmse = 0.88843\n",
      "\n",
      " 88%|████████▊ | 4402/5000 [08:04<01:03,  9.38it/s]\n",
      " running test-rmse = 4.41348\n",
      "running train-rmse = 0.95113\n",
      "\n",
      " 92%|█████████▏| 4602/5000 [08:26<00:43,  9.24it/s]\n",
      " running test-rmse = 1.05344\n",
      "running train-rmse = 0.93729\n",
      "\n",
      " 96%|█████████▌| 4802/5000 [08:47<00:21,  9.18it/s]\n",
      " running test-rmse = 1.05219\n",
      "running train-rmse = 1.02182\n",
      "\n",
      "100%|██████████| 5000/5000 [09:09<00:00,  9.11it/s]\n",
      "  0%|          | 1/5000 [00:00<09:08,  9.11it/s]\n",
      "\n",
      " final test rmse = 3.86695\n",
      "\n",
      "\n",
      " final test rmse = 3.86695\n",
      "with lr, rmse is 1.49559\n",
      "with elanet, rmse is 1.39805\n",
      "\n",
      " running test-rmse = 1.66884\n",
      "running train-rmse = 0.97745\n",
      "\n",
      "  4%|▍         | 202/5000 [00:21<08:43,  9.16it/s]\n",
      " running test-rmse = 1.50735\n",
      "running train-rmse = 0.53662\n",
      "\n",
      "  8%|▊         | 402/5000 [00:43<08:18,  9.22it/s]\n",
      " running test-rmse = 1.47908\n",
      "running train-rmse = 0.54218\n",
      "\n",
      " 12%|█▏        | 602/5000 [01:04<07:49,  9.36it/s]\n",
      " running test-rmse = 1.50063\n",
      "running train-rmse = 0.58631\n",
      "\n",
      " 16%|█▌        | 802/5000 [01:26<07:33,  9.26it/s]\n",
      " running test-rmse = 1.44408\n",
      "running train-rmse = 0.63293\n",
      "\n",
      " 20%|██        | 1002/5000 [01:48<07:00,  9.50it/s]\n",
      " running test-rmse = 1.53050\n",
      "running train-rmse = 0.62311\n",
      "\n",
      " 24%|██▍       | 1202/5000 [02:10<07:13,  8.77it/s]\n",
      " running test-rmse = 1.38089\n",
      "running train-rmse = 0.63676\n",
      "\n",
      " 28%|██▊       | 1402/5000 [02:32<06:13,  9.64it/s]\n",
      " running test-rmse = 1.37800\n",
      "running train-rmse = 0.61548\n",
      "\n",
      " 32%|███▏      | 1602/5000 [02:53<05:56,  9.53it/s]\n",
      " running test-rmse = 1.48777\n",
      "running train-rmse = 0.61788\n",
      "\n",
      " 36%|███▌      | 1802/5000 [03:14<05:54,  9.03it/s]\n",
      " running test-rmse = 1.35338\n",
      "running train-rmse = 0.63152\n",
      "\n",
      " 40%|████      | 2002/5000 [03:38<06:23,  7.83it/s]\n",
      " running test-rmse = 1.47441\n",
      "running train-rmse = 0.71410\n",
      "\n",
      " 44%|████▍     | 2202/5000 [04:02<04:47,  9.72it/s]\n",
      " running test-rmse = 1.43096\n",
      "running train-rmse = 0.60473\n",
      "\n",
      " 48%|████▊     | 2402/5000 [04:23<04:26,  9.75it/s]\n",
      " running test-rmse = 1.54335\n",
      "running train-rmse = 0.58781\n",
      "\n",
      " 52%|█████▏    | 2602/5000 [04:44<04:23,  9.10it/s]\n",
      " running test-rmse = 1.44503\n",
      "running train-rmse = 0.58186\n",
      "\n",
      " 56%|█████▌    | 2803/5000 [05:05<03:38, 10.04it/s]\n",
      " running test-rmse = 1.36164\n",
      "running train-rmse = 0.61205\n",
      "\n",
      " 60%|██████    | 3003/5000 [05:26<03:15, 10.23it/s]\n",
      " running test-rmse = 1.42603\n",
      "running train-rmse = 0.60309\n",
      "\n",
      " 64%|██████▍   | 3201/5000 [05:46<02:55, 10.25it/s]\n",
      " running test-rmse = 1.34005\n",
      "running train-rmse = 0.59511\n",
      "\n",
      " 68%|██████▊   | 3402/5000 [06:07<02:36, 10.19it/s]\n",
      " running test-rmse = 1.46547\n",
      "running train-rmse = 0.63152\n",
      "\n",
      " 72%|███████▏  | 3602/5000 [06:28<02:25,  9.63it/s]\n",
      " running test-rmse = 1.40319\n",
      "running train-rmse = 0.57745\n",
      "\n",
      " 76%|███████▌  | 3803/5000 [06:48<01:55, 10.40it/s]\n",
      " running test-rmse = 1.45353\n",
      "running train-rmse = 0.59907\n",
      "\n",
      " 80%|████████  | 4002/5000 [07:08<01:37, 10.27it/s]\n",
      " running test-rmse = 1.43451\n",
      "running train-rmse = 0.56014\n",
      "\n",
      " 84%|████████▍ | 4202/5000 [07:28<01:17, 10.35it/s]\n",
      " running test-rmse = 1.36848\n",
      "running train-rmse = 0.61729\n",
      "\n",
      " 88%|████████▊ | 4402/5000 [07:48<00:58, 10.23it/s]\n",
      " running test-rmse = 1.44360\n",
      "running train-rmse = 0.54277\n",
      "\n",
      " 92%|█████████▏| 4602/5000 [08:08<00:40,  9.93it/s]\n",
      " running test-rmse = 1.40426\n",
      "running train-rmse = 0.59688\n",
      "\n",
      " 96%|█████████▌| 4801/5000 [08:29<00:19, 10.08it/s]\n",
      " running test-rmse = 1.59512\n",
      "running train-rmse = 0.60737\n",
      "\n",
      "100%|██████████| 5000/5000 [08:49<00:00,  9.44it/s]\n",
      "  0%|          | 1/5000 [00:00<08:58,  9.28it/s]\n",
      "\n",
      " final test rmse = 1.39491\n",
      "\n",
      "\n",
      " final test rmse = 1.39491\n",
      "with lr, rmse is 1.29466\n",
      "with elanet, rmse is 1.21015\n",
      "\n",
      " running test-rmse = 1.11467\n",
      "running train-rmse = 0.76366\n",
      "\n",
      "  4%|▍         | 202/5000 [00:21<08:31,  9.38it/s]\n",
      " running test-rmse = 1.29558\n",
      "running train-rmse = 0.88298\n",
      "\n",
      "  8%|▊         | 402/5000 [00:42<08:18,  9.22it/s]\n",
      " running test-rmse = 1.06594\n",
      "running train-rmse = 0.92796\n",
      "\n",
      " 12%|█▏        | 602/5000 [01:03<07:08, 10.26it/s]\n",
      " running test-rmse = 1.07881\n",
      "running train-rmse = 0.93683\n",
      "\n",
      " 16%|█▌        | 802/5000 [01:23<07:14,  9.65it/s]\n",
      " running test-rmse = 1.19314\n",
      "running train-rmse = 0.86811\n",
      "\n",
      " 20%|██        | 1003/5000 [01:43<06:54,  9.65it/s]\n",
      " running test-rmse = 1.26759\n",
      "running train-rmse = 0.90466\n",
      "\n",
      " 24%|██▍       | 1202/5000 [02:04<06:25,  9.86it/s]\n",
      " running test-rmse = 1.23356\n",
      "running train-rmse = 0.83663\n",
      "\n",
      " 28%|██▊       | 1403/5000 [02:23<05:47, 10.34it/s]\n",
      " running test-rmse = 1.18006\n",
      "running train-rmse = 0.80772\n",
      "\n",
      " 32%|███▏      | 1602/5000 [02:44<06:14,  9.08it/s]\n",
      " running test-rmse = 1.39858\n",
      "running train-rmse = 0.85987\n",
      "\n",
      " 36%|███▌      | 1802/5000 [03:05<05:22,  9.92it/s]\n",
      " running test-rmse = 1.20085\n",
      "running train-rmse = 0.94433\n",
      "\n",
      " 40%|████      | 2002/5000 [03:25<04:59, 10.01it/s]\n",
      " running test-rmse = 1.21348\n",
      "running train-rmse = 0.85056\n",
      "\n",
      " 44%|████▍     | 2202/5000 [03:46<04:58,  9.37it/s]\n",
      " running test-rmse = 1.13313\n",
      "running train-rmse = 0.95228\n",
      "\n",
      " 48%|████▊     | 2403/5000 [04:06<04:18, 10.06it/s]\n",
      " running test-rmse = 1.32937\n",
      "running train-rmse = 0.86278\n",
      "\n",
      " 52%|█████▏    | 2603/5000 [04:26<03:58, 10.05it/s]\n",
      " running test-rmse = 1.20087\n",
      "running train-rmse = 0.93018\n",
      "\n",
      " 56%|█████▌    | 2802/5000 [04:46<03:46,  9.72it/s]\n",
      " running test-rmse = 1.40923\n",
      "running train-rmse = 0.87625\n",
      "\n",
      " 60%|██████    | 3002/5000 [05:07<03:36,  9.25it/s]\n",
      " running test-rmse = 1.11628\n",
      "running train-rmse = 0.94921\n",
      "\n",
      " 64%|██████▍   | 3201/5000 [05:27<03:11,  9.38it/s]\n",
      " running test-rmse = 1.00384\n",
      "running train-rmse = 0.86734\n",
      "\n",
      " 68%|██████▊   | 3402/5000 [05:48<02:42,  9.80it/s]\n",
      " running test-rmse = 1.38888\n",
      "running train-rmse = 0.90293\n",
      "\n",
      " 72%|███████▏  | 3602/5000 [06:08<02:22,  9.83it/s]\n",
      " running test-rmse = 1.29342\n",
      "running train-rmse = 0.96702\n",
      "\n",
      " 76%|███████▌  | 3802/5000 [06:29<02:09,  9.28it/s]\n",
      " running test-rmse = 1.19472\n",
      "running train-rmse = 0.82123\n",
      "\n",
      " 80%|████████  | 4002/5000 [06:50<01:42,  9.69it/s]\n",
      " running test-rmse = 1.23764\n",
      "running train-rmse = 0.91935\n",
      "\n",
      " 84%|████████▍ | 4202/5000 [07:11<01:26,  9.21it/s]\n",
      " running test-rmse = 1.29672\n",
      "running train-rmse = 0.91417\n",
      "\n",
      " 88%|████████▊ | 4402/5000 [07:31<01:05,  9.15it/s]\n",
      " running test-rmse = 1.07984\n",
      "running train-rmse = 0.99376\n",
      "\n",
      " 92%|█████████▏| 4603/5000 [07:52<00:38, 10.27it/s]\n",
      " running test-rmse = 1.07298\n",
      "running train-rmse = 0.92128\n",
      "\n",
      " 96%|█████████▌| 4802/5000 [08:12<00:20,  9.67it/s]\n",
      " running test-rmse = 1.38080\n",
      "running train-rmse = 0.94232\n",
      "\n",
      "100%|██████████| 5000/5000 [08:32<00:00,  9.75it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n",
      "\n",
      " final test rmse = 1.15481\n",
      "\n",
      "\n",
      " final test rmse = 1.15481\n",
      "with lr, rmse is 1.03112\n",
      "with elanet, rmse is 0.84005\n",
      "\n",
      " running test-rmse = 1.16266\n",
      "running train-rmse = 1.63585\n",
      "\n",
      "  4%|▍         | 202/5000 [00:20<08:02,  9.94it/s]\n",
      " running test-rmse = 1.32542\n",
      "running train-rmse = 0.93444\n",
      "\n",
      "  8%|▊         | 402/5000 [00:41<08:33,  8.95it/s]\n",
      " running test-rmse = 1.17302\n",
      "running train-rmse = 0.92895\n",
      "\n",
      " 12%|█▏        | 602/5000 [01:01<07:15, 10.10it/s]\n",
      " running test-rmse = 1.13766\n",
      "running train-rmse = 0.96756\n",
      "\n",
      " 16%|█▌        | 801/5000 [01:22<07:23,  9.47it/s]\n",
      " running test-rmse = 1.08050\n",
      "running train-rmse = 1.02560\n",
      "\n",
      " 20%|██        | 1002/5000 [01:43<07:23,  9.02it/s]\n",
      " running test-rmse = 0.97129\n",
      "running train-rmse = 0.92599\n",
      "\n",
      " 24%|██▍       | 1202/5000 [02:04<06:28,  9.77it/s]\n",
      " running test-rmse = 0.97310\n",
      "running train-rmse = 0.91152\n",
      "\n",
      " 28%|██▊       | 1402/5000 [02:25<06:04,  9.87it/s]\n",
      " running test-rmse = 1.09423\n",
      "running train-rmse = 0.97564\n",
      "\n",
      " 32%|███▏      | 1602/5000 [02:46<06:07,  9.25it/s]\n",
      " running test-rmse = 1.19204\n",
      "running train-rmse = 0.83853\n",
      "\n",
      " 36%|███▌      | 1802/5000 [03:07<05:14, 10.16it/s]\n",
      " running test-rmse = 0.91927\n",
      "running train-rmse = 0.91940\n",
      "\n",
      " 40%|████      | 2002/5000 [03:28<05:16,  9.48it/s]\n",
      " running test-rmse = 1.47220\n",
      "running train-rmse = 0.93722\n",
      "\n",
      " 44%|████▍     | 2202/5000 [03:49<04:52,  9.57it/s]\n",
      " running test-rmse = 1.29377\n",
      "running train-rmse = 0.94684\n",
      "\n",
      " 48%|████▊     | 2402/5000 [04:09<04:45,  9.11it/s]\n",
      " running test-rmse = 0.93325\n",
      "running train-rmse = 0.89381\n",
      "\n",
      " 52%|█████▏    | 2602/5000 [04:31<04:24,  9.07it/s]\n",
      " running test-rmse = 0.96683\n",
      "running train-rmse = 0.92048\n",
      "\n",
      " 56%|█████▌    | 2802/5000 [04:52<03:56,  9.31it/s]\n",
      " running test-rmse = 0.88811\n",
      "running train-rmse = 0.99980\n",
      "\n",
      " 60%|██████    | 3002/5000 [05:13<03:34,  9.30it/s]\n",
      " running test-rmse = 0.81002\n",
      "running train-rmse = 0.88188\n",
      "\n",
      " 64%|██████▍   | 3202/5000 [05:33<03:08,  9.54it/s]\n",
      " running test-rmse = 0.96196\n",
      "running train-rmse = 1.03400\n",
      "\n",
      " 68%|██████▊   | 3402/5000 [05:54<02:57,  9.02it/s]\n",
      " running test-rmse = 1.13543\n",
      "running train-rmse = 1.01885\n",
      "\n",
      " 72%|███████▏  | 3602/5000 [06:16<02:37,  8.88it/s]\n",
      " running test-rmse = 4.05606\n",
      "running train-rmse = 1.01719\n",
      "\n",
      " 76%|███████▌  | 3802/5000 [06:39<02:18,  8.67it/s]\n",
      " running test-rmse = 0.92027\n",
      "running train-rmse = 0.99515\n",
      "\n",
      " 80%|████████  | 4002/5000 [07:00<01:48,  9.16it/s]\n",
      " running test-rmse = 0.93177\n",
      "running train-rmse = 0.90493\n",
      "\n",
      " 84%|████████▍ | 4201/5000 [07:21<01:26,  9.25it/s]\n",
      " running test-rmse = 0.92972\n",
      "running train-rmse = 1.00840\n",
      "\n",
      " 88%|████████▊ | 4402/5000 [07:42<01:01,  9.66it/s]\n",
      " running test-rmse = 1.21467\n",
      "running train-rmse = 0.85820\n",
      "\n",
      " 92%|█████████▏| 4602/5000 [08:03<00:42,  9.29it/s]\n",
      " running test-rmse = 2.22358\n",
      "running train-rmse = 0.96522\n",
      "\n",
      " 96%|█████████▌| 4802/5000 [08:24<00:19, 10.03it/s]\n",
      " running test-rmse = 1.45010\n",
      "running train-rmse = 0.93693\n",
      "\n",
      "100%|██████████| 5000/5000 [08:46<00:00,  9.50it/s]\n",
      "\n",
      " final test rmse = 4.61573\n",
      "\n",
      "\n",
      " final test rmse = 4.61573\n",
      "lr_rmse_mean: 1.2344,lr_rmse_std: 0.1603 \n",
      "esnet_rmse_mean: 1.0427,esnet_rmse_std: 0.2226 \n",
      "ours_rmse_mean: 2.4078,ours_rmse_std: 1.5207 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split train & test\n",
    "N=5\n",
    "lr_rmse = np.zeros(N)\n",
    "esnet_rmse = np.zeros(N)\n",
    "ours_rmse = np.zeros(N)\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, Y.squeeze(),test_size=0.30)\n",
    "    data_dict = {'X_tr':X_train, 'y_tr':y_train, 'X_test':X_test, 'y_test':y_test}      \n",
    "    w_lr,lr_rmse[i] = baseline_lr(data_dict)\n",
    "    w_es,esnet_rmse[i] = baseline_esnet(data_dict)\n",
    "    model = Gibbs_sampling(data_dict,get_init_paras(w_lr), hyper_paras)\n",
    "    model.model_run()\n",
    "    ours_rmse[i] = model.model_test()\n",
    "\n",
    "print('lr_rmse_mean: %.4f,lr_rmse_std: %.4f '%(lr_rmse.mean(),lr_rmse.std() ) )\n",
    "print('esnet_rmse_mean: %.4f,esnet_rmse_std: %.4f '%(esnet_rmse.mean(),esnet_rmse.std() ) )\n",
    "print('ours_rmse_mean: %.4f,ours_rmse_std: %.4f '%(ours_rmse.mean(),ours_rmse.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lr_rmse_mean: 1.3432,lr_rmse_std: 0.1101 \nesnet_rmse_mean: 1.1714,esnet_rmse_std: 0.2027 \nours_rmse_mean: 1.1854,ours_rmse_std: 0.1600 \n"
     ]
    }
   ],
   "source": [
    "print('lr_rmse_mean: %.4f,lr_rmse_std: %.4f '%(lr_rmse[[0,2,3]].mean(),lr_rmse[[0,2,3]].std() ) )\n",
    "print('esnet_rmse_mean: %.4f,esnet_rmse_std: %.4f '%(esnet_rmse[[0,2,3]].mean(),esnet_rmse[[0,2,3]].std() ) )\n",
    "print('ours_rmse_mean: %.4f,ours_rmse_std: %.4f '%(ours_rmse[[0,2,3]].mean(),ours_rmse[[0,2,3]].std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([ 0, 24, 26], dtype=int64),)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "model.s_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "with elanet, rmse is 0.92433\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "clf = ElasticNet(alpha = 0.2,l1_ratio =0.1, fit_intercept=False)\n",
    "clf.fit(data_dict['X_tr'], data_dict['y_tr'])\n",
    "predict = clf.predict(data_dict['X_test']).squeeze()\n",
    "rmse = np.sqrt(np.mean((predict-data_dict['y_test'].squeeze())**2))\n",
    "print('with elanet, rmse is %.5f'%(rmse))\n",
    "len(clf.coef_[clf.coef_>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.02290048, -0.00710251, -0.        ,\n",
       "       -0.        , -0.00906848, -0.        , -0.00073689, -0.        ,\n",
       "       -0.01301175,  0.        ,  0.        , -0.        ,  0.04461527,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.04582258,\n",
       "       -0.        , -0.        ,  0.        , -0.00123495, -0.00794974,\n",
       "       -0.01668715, -0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.05419244,  0.        , -0.        , -0.05123613,  0.00319208,\n",
       "       -0.11459053,  0.05525388,  0.        ,  0.02155437,  0.        ,\n",
       "        0.02790468,  0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.00617801,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.03845294,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.04606137, -0.        , -0.04967678,\n",
       "       -0.0510887 , -0.03915234,  0.        , -0.024777  ,  0.03848892,\n",
       "       -0.02260924,  0.0034599 , -0.01779437, -0.15154597,  0.08939166,\n",
       "        0.        ,  0.07388046,  0.00081879,  0.01024509,  0.        ,\n",
       "        0.00782891,  0.        ,  0.03999645,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.0009416 , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.03962395, -0.01585616, -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.00148079,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.03311627,  0.02364622,  0.02364671,  0.        ,\n",
       "        0.02881839, -0.        ,  0.01989795,  0.        ,  0.        ,\n",
       "        0.        ,  0.02707263, -0.03641279, -0.        ,  0.00471888,\n",
       "        0.01585947,  0.        ,  0.        ,  0.01715877,  0.        ,\n",
       "        0.00780515,  0.02258382, -0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.05046656,\n",
       "        0.00712515, -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.00911781,  0.        ,  0.0308587 ,  0.        ,  0.12240987,\n",
       "        0.        , -0.        ,  0.01683971, -0.        , -0.0175792 ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.02226112,\n",
       "        0.        , -0.        , -0.01948966, -0.        , -0.00655018,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.01214122,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.00299604,  0.        ,\n",
       "        0.        ,  0.01184025, -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.12643198, -0.        ,  0.03102914,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        , -0.02904414,\n",
       "       -0.        , -0.        ,  0.00792165, -0.        , -0.        ,\n",
       "        0.        , -0.01458936, -0.        , -0.00493862, -0.        ,\n",
       "       -0.        , -0.03742177, -0.01490766, -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.02930586,\n",
       "       -0.        , -0.0035898 ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.04591052, -0.00539842, -0.        , -0.00423613,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.0044806 , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.01892325, -0.        , -0.        ,\n",
       "       -0.17849845,  0.01033251,  0.        , -0.        ,  0.02962639,\n",
       "        0.        , -0.        , -0.00563414, -0.        ,  0.        ,\n",
       "        0.00985186,  0.        ,  0.00467639,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.04907754, -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.00513774, -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.04629642,  0.02493082,  0.02733485,  0.        ,\n",
       "        0.05399662,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.03792165,\n",
       "        0.10768513,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.01404093,  0.        ,  0.        ,  0.        ,  0.02202576,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.01181202, -0.        , -0.        , -0.01236265, -0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.00821121,\n",
       "       -0.02238252, -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.00168103,  0.        , -0.        , -0.10912436,  0.        ,\n",
       "       -0.        ,  0.        , -0.06424858, -0.        , -0.        ,\n",
       "        0.10415519,  0.04376389,  0.01697914,  0.        ,  0.        ,\n",
       "       -0.01312378,  0.03672591,  0.        , -0.03417803,  0.        ,\n",
       "        0.        ,  0.02856535, -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.02499551,  0.        , -0.        ,\n",
       "       -0.02190513,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.05397044, -0.        , -0.        , -0.04215188,\n",
       "       -0.04784062, -0.        , -0.01406243, -0.        , -0.02008678,\n",
       "       -0.03620158, -0.        , -0.01014058, -0.08876233,  0.11709101,\n",
       "       -0.03473525, -0.02286894,  0.        ,  0.05516396, -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.05392373,  0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.02025788,  0.        ,  0.        ,\n",
       "        0.02495222,  0.02837319,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.03277729,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.01735679, -0.01053047, -0.        ,\n",
       "       -0.        ,  0.        ,  0.10418004,  0.02373764,  0.        ,\n",
       "       -0.        , -0.0439794 , -0.        ,  0.03669145,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.00784269,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.02417521,  0.        ,  0.01993967,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.02476074,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.0255349 ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.02190399,\n",
       "       -0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.08300116, -0.01377398,  0.06784959, -0.0311479 ,\n",
       "       -0.06275233, -0.03652226, -0.        ,  0.        ,  0.        ,\n",
       "        0.06701617,  0.02434526, -0.        , -0.        , -0.        ,\n",
       "       -0.01651747, -0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.04118781,\n",
       "       -0.00059209, -0.        ,  0.        ,  0.00142739,  0.01374944,\n",
       "       -0.02384531, -0.15420052, -0.06653698, -0.        ,  0.        ,\n",
       "        0.1033298 ,  0.06381967,  0.        ,  0.03097225, -0.        ,\n",
       "       -0.01842837, -0.03546417, -0.00226203, -0.        ,  0.        ,\n",
       "       -0.00353995, -0.00972607, -0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.05025683,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.00739962,  0.        ,  0.        ,  0.02112023,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.0034799 ,\n",
       "       -0.00728382,  0.        , -0.03972991, -0.        , -0.00405776,\n",
       "       -0.        , -0.0639066 , -0.11904907, -0.        ,  0.00150147,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.03250672,\n",
       "        0.00436798,  0.        ,  0.        ,  0.0348344 ,  0.08700881,\n",
       "        0.        ,  0.00404911,  0.        ,  0.        ,  0.00285669,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.02295848,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.01329357, -0.        ,  0.0257707 ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.00984572,  0.        ,  0.03657566,\n",
       "        0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.08141421,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "w_es"
   ]
  }
 ]
}