{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd06ee6abd9b0d44c31108424068010123bef58fe758ec0be567da6fa0551538e82",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cb03ba31accc2f25e369d1aa3152af3569125a23627c23321dfacf015e0ff229"
   }
  },
  "interpreter": {
   "hash": "cb03ba31accc2f25e369d1aa3152af3569125a23627c23321dfacf015e0ff229"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy\n",
    "# import xlrd \n",
    "import sklearn\n",
    "\n",
    "from Gibbs_model_probit import Gibbs_sampling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import multivariate_normal\n",
    "from utils import baseline_lr,baseline_esnet,baseline_justmean\n",
    "from utils import baseline_LogitElsnet,baseline_justmode,baseline_random,baseline_LogitLR,baseline_RanForest,baseline_Gibbs_zhe\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import binom \n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import trange\n",
    "from Gibbd_zhe import GibbsSampling3\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# data_loading \n",
    "np.random.seed(123)\n",
    "data_table = pd.read_csv('../data/processed/all_feature_p1_lip_specie.csv')\n",
    "target = '1= death; 0=alive'\n",
    "\n",
    "\n",
    "# normalization\n",
    "\n",
    "# min-max\n",
    "df = data_table[target]\n",
    "\n",
    "\n",
    "# check nan\n",
    "data_table[target].isnull().values.any()\n",
    "data_table.fillna(data_table.mean(), inplace=True) # fill nan with column mean"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "all_feature = data_table.iloc[:,1:-7]\n",
    "#lip_feature = data_table.iloc[:,1:775]#for p2\n",
    "# gene_feature = data_table.iloc[:,1:-372]#for p1\n",
    "\n",
    "Y = data_table[target].values\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_feature"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "feature_names = list(all_feature.columns)\n",
    "K_lip=41 # group number, from data process notebook\n",
    "K_gene=3\n",
    "K = K_lip + K_gene \n",
    "\n",
    "group_ind_dict = {}\n",
    "group_ind = []\n",
    "group_ind_concat = []\n",
    "for i in range(K_lip):\n",
    "    group_ind_dict['lip'+'_'+str(i)] = []\n",
    "for i in range(K_gene):\n",
    "    group_ind_dict['gene'+'_'+str(i)] = []\n",
    "\n",
    "for name in feature_names:\n",
    "    cate = name.split('_')[0]\n",
    "    id = name.split('_')[-1]\n",
    "    group_ind_dict[cate+'_'+id].append(name)\n",
    "\n",
    "for i in range(K_lip):\n",
    "    group_ind.append(group_ind_dict['lip'+'_'+str(i)])\n",
    "    group_ind_concat = group_ind_concat + group_ind_dict['lip'+'_'+str(i)]\n",
    "\n",
    "for i in range(K_gene):\n",
    "    group_ind.append(group_ind_dict['gene'+'_'+str(i)])\n",
    "    group_ind_concat = group_ind_concat + group_ind_dict['gene'+'_'+str(i)]\n",
    "\n",
    "# group_ind"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# re-arrange the features of X based on the group split order\n",
    "X_new = all_feature[group_ind_concat].values\n",
    "\n",
    "N_sample, _ = X_new.shape\n",
    "# add all-one column at the last \n",
    "bias_col = np.ones(N_sample).reshape((N_sample,1))\n",
    "X_new = np.concatenate((X_new,bias_col),axis=1)\n",
    "\n",
    "print(X_new.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(71, 1140)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "\n",
    "# init hyper-parameters\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "r0 = 1e-6\n",
    "r1 = 100.0\n",
    "a0 = 1.0\n",
    "b0 = 1.0\n",
    "JITTER = 1e-3\n",
    "\n",
    "INTERVAL = 1\n",
    "VALITA_INTERVAL = 10\n",
    "BURNING = 20\n",
    "MAX_NUMBER = 100\n",
    "\n",
    "hyper_paras = {'INTERVAL':INTERVAL, 'BURNING':BURNING,'MAX_NUMBER':MAX_NUMBER,'VALITA_INTERVAL':VALITA_INTERVAL,\n",
    "'alpha':alpha, 'beta':beta,'r0':r0,'r1':r1,'JITTER':JITTER}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def zero_init_paras():\n",
    "    z_array_init = np.ones(K) #np.random.binomial(size=K, n=1, p= alpha)\n",
    "    s_list_init = [np.ones(len(item)) for item in group_ind]\n",
    "    # s_list_init = [np.ones(len(item)) for item in group_ind]#[np.random.binomial(size=len(item), n=1, p= beta) for item in group_ind]\n",
    "    b_init = 0.0#np.random.normal(loc=0.0, scale=r1,size=None)\n",
    "    # tau_init = 1.0#np.random.gamma(shape=alpha, scale=1.0/beta, size=None)\n",
    "\n",
    "    W_init = [np.zeros(len(item)) for item in group_ind]\n",
    "\n",
    "    init_paras = {'z':z_array_init, 's':s_list_init, 'b':b_init,  'W':W_init,'a0':a0,'b0':b0}\n",
    "    return init_paras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# init parameters with lr_result\n",
    "def get_init_paras(w_lr):\n",
    "    z_array_init = np.ones(K) #np.random.binomial(size=K, n=1, p= alpha)\n",
    "    s_list_init = [np.ones(len(item)) for item in group_ind]#[np.random.binomial(size=len(item), n=1, p= beta) for item in group_ind]\n",
    "    b_init = w_lr[-1]#np.random.normal(loc=0.0, scale=r1,size=None)\n",
    "    # tau_init = 1.0#np.random.gamma(shape=alpha, scale=1.0/beta, size=None)\n",
    "\n",
    "    W_init = []\n",
    "    offset=0\n",
    "    for i in range(K):\n",
    "        # mask1 = 1-z_array_init[i] * s_list_init[i]\n",
    "        # mask2 = z_array_init[i] * s_list_init[i]\n",
    "        # spike = np.random.normal(loc=0.0, scale=r0,size=len(s_list_init[i]))\n",
    "        # slab = np.random.normal(loc=0.0, scale=r1,size=len(s_list_init[i]))\n",
    "        # W_group = spike * mask1 + slab * mask2\n",
    "\n",
    "        \n",
    "        group_len = len(s_list_init[i])\n",
    "        W_group= w_lr[offset:offset+group_len]\n",
    "        offset = offset + group_len\n",
    "        W_init.append(W_group)\n",
    "\n",
    "    init_paras = {'z':z_array_init, 's':s_list_init, 'b':b_init,  'W':W_init,'a0':a0,'b0':b0}\n",
    "    return init_paras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "\n",
    "N = 3\n",
    "lr_acc = np.zeros(N)\n",
    "rf_acc = np.zeros(N)\n",
    "esnet_acc = np.zeros(N)\n",
    "mode_acc = np.zeros(N)\n",
    "random_acc = np.zeros(N)\n",
    "ours_acc = np.zeros(N)\n",
    "zhe_gibs_acc = np.zeros(N)\n",
    "\n",
    "lr_auc = np.zeros(N)\n",
    "rf_auc = np.zeros(N)\n",
    "esnet_auc = np.zeros(N)\n",
    "ours_auc = np.zeros(N)\n",
    "zhe_gibs_auc = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, Y.squeeze(),test_size=0.3)\n",
    "\n",
    "    data_dict = {'X_tr':X_train, 'y_tr':y_train, 'X_test':X_test, 'y_test':y_test}  \n",
    "    dict_lr = baseline_LogitLR(data_dict)\n",
    "    dict_els = baseline_LogitElsnet(data_dict)\n",
    "    dict_rf = baseline_RanForest(data_dict)\n",
    "    dict_mode = baseline_justmode(data_dict)\n",
    "    dict_random = baseline_random(data_dict)\n",
    "    dict_gibbs_zhe = baseline_Gibbs_zhe(data_dict,hyper_paras)\n",
    "\n",
    "    # model = Gibbs_sampling(data_dict,get_init_paras(dict_lr['clf'].coef_.squeeze()), hyper_paras)\n",
    "    model = Gibbs_sampling(data_dict,zero_init_paras(), hyper_paras)\n",
    "    dict_ours = model.model_run()\n",
    "\n",
    "    lr_acc[i] = dict_lr['acr']\n",
    "    esnet_acc[i] = dict_els['acr']\n",
    "    rf_acc[i] = dict_rf['acr']\n",
    "    mode_acc[i] = dict_mode['acr']\n",
    "    random_acc[i] = dict_random['acr']\n",
    "    ours_acc[i] = dict_ours['acr']\n",
    "    zhe_gibs_acc[i] = dict_gibbs_zhe['acr']\n",
    "\n",
    "    lr_auc[i] = dict_lr['auc']\n",
    "    rf_auc[i] = dict_rf['auc']\n",
    "    esnet_auc[i] = dict_els['auc']\n",
    "    ours_auc[i] = dict_ours['auc']\n",
    "    zhe_gibs_auc[i] = dict_gibbs_zhe['auc']\n",
    "\n",
    "print('\\n\\nours_acr_mean: %.4f,ours_acr_std: %.4f '%( ours_acc.mean(), ours_acc.std() ) )\n",
    "print('gibbs_zhe_acr_mean: %.4f,gibbs_zhe_acr_std: %.4f '%(zhe_gibs_acc.mean(),zhe_gibs_acc.std() ) )\n",
    "print('lr_acr_mean: %.4f,lr_acr_std: %.4f '%(lr_acc.mean(),lr_acc.std() ) )\n",
    "print('esnet_acr_mean: %.4f,esnet_acr_std: %.4f '%(esnet_acc.mean(),esnet_acc.std() ) )\n",
    "print('rf_acr_mean: %.4f,rf_acr_mean: %.4f '%(rf_acc.mean(),rf_acc.std() ) )\n",
    "print('just-mode_acr_mean: %.4f,mode_acr_std: %.4f '%(mode_acc.mean(),mode_acc.std() ) )\n",
    "print('just-random_acr_mean: %.4f,just-random_acr_std: %.4f '%(random_acc.mean(),random_acc.std() ) )\n",
    "\n",
    "\n",
    "print('\\nours_AUC_mean: %.4f,ours_AUC_std: %.4f '%(ours_auc.mean(),ours_auc.std() ) )\n",
    "print('gibbs_zhe_AUC_mean: %.4f,gibbs_zhe_AUC_std: %.4f '%(zhe_gibs_auc.mean(),zhe_gibs_auc.std() ) )\n",
    "print('lr_AUC_mean: %.4f,lr_AUC_std: %.4f '%(lr_auc.mean(),lr_auc.std() ) )\n",
    "print('esnet_AUC_mean: %.4f,esnet_AUC_mean: %.4f '%(esnet_auc.mean(),esnet_auc.std() ) )\n",
    "print('rf_AUC_mean: %.4f,rf_AUC_std: %.4f '%(rf_auc.mean(),rf_auc.std() ) )\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 3.20it/s]ours:W_max:31.4637,W_min:-28.7030\n",
      " 87%|████████▋ | 87/100 [00:27<00:04,  3.24it/s]ours:W_max:37.8530,W_min:-28.1590\n",
      " 88%|████████▊ | 88/100 [00:27<00:03,  3.20it/s]ours:W_max:35.7719,W_min:-33.1187\n",
      " 89%|████████▉ | 89/100 [00:27<00:03,  3.22it/s]ours:W_max:26.7341,W_min:-42.8702\n",
      " 90%|█████████ | 90/100 [00:27<00:03,  3.18it/s]ours:W_max:34.7218,W_min:-29.4597\n",
      "\n",
      " running test-auc = 0.45089\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 91%|█████████ | 91/100 [00:28<00:02,  3.17it/s]ours:W_max:26.1862,W_min:-29.9596\n",
      " 92%|█████████▏| 92/100 [00:28<00:02,  3.11it/s]ours:W_max:37.4203,W_min:-29.2417\n",
      " 93%|█████████▎| 93/100 [00:28<00:02,  3.11it/s]ours:W_max:30.6178,W_min:-32.5564\n",
      " 94%|█████████▍| 94/100 [00:29<00:01,  3.17it/s]ours:W_max:29.2723,W_min:-29.7473\n",
      " 95%|█████████▌| 95/100 [00:29<00:01,  3.13it/s]ours:W_max:34.0698,W_min:-31.3444\n",
      " 96%|█████████▌| 96/100 [00:29<00:01,  3.17it/s]ours:W_max:27.8414,W_min:-34.8666\n",
      " 97%|█████████▋| 97/100 [00:30<00:00,  3.18it/s]ours:W_max:30.8483,W_min:-30.1785\n",
      " 98%|█████████▊| 98/100 [00:30<00:00,  3.18it/s]ours:W_max:30.4919,W_min:-35.9458\n",
      " 99%|█████████▉| 99/100 [00:30<00:00,  3.19it/s]ours:W_max:30.1542,W_min:-36.8937\n",
      "100%|██████████| 100/100 [00:31<00:00,  3.21it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.75000, acr = 0.50000\n",
      "\n",
      "\n",
      " final test auc_full = 0.71429, acr_full = 0.50000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.772727, auc is 0.783333,  fpr is 0.300000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.772727, auc is 0.783333,  fpr is 0.300000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.681818, auc is 0.720833,  fpr is 0.200000\n",
      "for justmode: accuracy is 0.454545, fpr is 0.000000\n",
      "for random-guess: accuracy is 0.545455, fpr is 0.300000\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]zhe:W_max:25.5884,W_min:-35.7309\n",
      "  1%|          | 1/100 [00:00<00:28,  3.42it/s]zhe:W_max:36.2556,W_min:-30.9259\n",
      "  2%|▏         | 2/100 [00:00<00:28,  3.40it/s]zhe:W_max:32.0294,W_min:-32.2524\n",
      "  3%|▎         | 3/100 [00:00<00:29,  3.32it/s]zhe:W_max:33.1003,W_min:-36.3805\n",
      "  4%|▍         | 4/100 [00:01<00:29,  3.29it/s]zhe:W_max:31.7677,W_min:-30.7078\n",
      "  5%|▌         | 5/100 [00:01<00:28,  3.28it/s]zhe:W_max:28.4935,W_min:-31.4617\n",
      "  6%|▌         | 6/100 [00:01<00:28,  3.26it/s]zhe:W_max:32.8609,W_min:-30.0613\n",
      "  7%|▋         | 7/100 [00:02<00:27,  3.34it/s]zhe:W_max:29.8007,W_min:-29.8497\n",
      "  8%|▊         | 8/100 [00:02<00:27,  3.32it/s]zhe:W_max:31.3024,W_min:-28.7090\n",
      "  9%|▉         | 9/100 [00:02<00:27,  3.30it/s]zhe:W_max:27.7661,W_min:-35.3952\n",
      " 10%|█         | 10/100 [00:03<00:26,  3.34it/s]zhe:W_max:32.7079,W_min:-29.5679\n",
      " 11%|█         | 11/100 [00:03<00:26,  3.35it/s]zhe:W_max:34.1627,W_min:-33.0140\n",
      " 12%|█▏        | 12/100 [00:03<00:26,  3.33it/s]zhe:W_max:30.5665,W_min:-26.3640\n",
      " 13%|█▎        | 13/100 [00:03<00:26,  3.29it/s]zhe:W_max:32.6810,W_min:-32.2646\n",
      " 14%|█▍        | 14/100 [00:04<00:26,  3.25it/s]zhe:W_max:31.0603,W_min:-31.5293\n",
      " 15%|█▌        | 15/100 [00:04<00:26,  3.22it/s]zhe:W_max:28.5574,W_min:-29.1771\n",
      " 16%|█▌        | 16/100 [00:04<00:25,  3.28it/s]zhe:W_max:28.7429,W_min:-32.1105\n",
      " 17%|█▋        | 17/100 [00:05<00:25,  3.29it/s]zhe:W_max:35.4237,W_min:-37.7339\n",
      " 18%|█▊        | 18/100 [00:05<00:25,  3.26it/s]zhe:W_max:34.8787,W_min:-29.4291\n",
      " 19%|█▉        | 19/100 [00:05<00:24,  3.29it/s]zhe:W_max:27.8513,W_min:-34.7481\n",
      " 20%|██        | 20/100 [00:06<00:24,  3.33it/s]zhe:W_max:31.6291,W_min:-32.3885\n",
      " 21%|██        | 21/100 [00:06<00:23,  3.34it/s]zhe:W_max:32.6708,W_min:-35.6819\n",
      " 22%|██▏       | 22/100 [00:06<00:23,  3.29it/s]zhe:W_max:29.0127,W_min:-28.4435\n",
      " 23%|██▎       | 23/100 [00:06<00:23,  3.27it/s]zhe:W_max:33.2921,W_min:-28.4950\n",
      " 24%|██▍       | 24/100 [00:07<00:23,  3.30it/s]zhe:W_max:34.7432,W_min:-37.0256\n",
      " 25%|██▌       | 25/100 [00:07<00:22,  3.37it/s]zhe:W_max:36.0114,W_min:-35.6071\n",
      " 26%|██▌       | 26/100 [00:07<00:21,  3.42it/s]zhe:W_max:32.9164,W_min:-31.4030\n",
      " 27%|██▋       | 27/100 [00:08<00:21,  3.39it/s]zhe:W_max:40.2708,W_min:-35.2974\n",
      " 28%|██▊       | 28/100 [00:08<00:21,  3.31it/s]zhe:W_max:32.2392,W_min:-32.3236\n",
      " 29%|██▉       | 29/100 [00:08<00:21,  3.26it/s]zhe:W_max:32.4860,W_min:-34.0703\n",
      " 30%|███       | 30/100 [00:09<00:20,  3.34it/s]zhe:W_max:32.6450,W_min:-30.4507\n",
      " 31%|███       | 31/100 [00:09<00:20,  3.33it/s]zhe:W_max:27.6569,W_min:-36.2530\n",
      " 32%|███▏      | 32/100 [00:09<00:20,  3.32it/s]zhe:W_max:31.0562,W_min:-37.4285\n",
      " 33%|███▎      | 33/100 [00:09<00:19,  3.38it/s]zhe:W_max:32.0228,W_min:-36.0593\n",
      " 34%|███▍      | 34/100 [00:10<00:19,  3.40it/s]zhe:W_max:34.0291,W_min:-31.7094\n",
      " 35%|███▌      | 35/100 [00:10<00:19,  3.35it/s]zhe:W_max:29.6628,W_min:-31.5528\n",
      " 36%|███▌      | 36/100 [00:10<00:19,  3.33it/s]zhe:W_max:33.3508,W_min:-31.8757\n",
      " 37%|███▋      | 37/100 [00:11<00:18,  3.34it/s]zhe:W_max:37.7915,W_min:-37.0222\n",
      " 38%|███▊      | 38/100 [00:11<00:18,  3.31it/s]zhe:W_max:27.5227,W_min:-32.5514\n",
      " 39%|███▉      | 39/100 [00:11<00:18,  3.29it/s]zhe:W_max:32.4943,W_min:-41.4094\n",
      " 40%|████      | 40/100 [00:12<00:17,  3.34it/s]zhe:W_max:31.4427,W_min:-29.5609\n",
      " 41%|████      | 41/100 [00:12<00:17,  3.33it/s]zhe:W_max:40.6501,W_min:-30.6525\n",
      " 42%|████▏     | 42/100 [00:12<00:17,  3.29it/s]zhe:W_max:31.4419,W_min:-34.3017\n",
      " 43%|████▎     | 43/100 [00:12<00:16,  3.36it/s]zhe:W_max:32.6798,W_min:-39.2248\n",
      " 44%|████▍     | 44/100 [00:13<00:16,  3.33it/s]zhe:W_max:26.6566,W_min:-35.7234\n",
      " 45%|████▌     | 45/100 [00:13<00:16,  3.30it/s]zhe:W_max:34.9403,W_min:-33.7952\n",
      " 46%|████▌     | 46/100 [00:13<00:16,  3.35it/s]zhe:W_max:31.6715,W_min:-38.8257\n",
      " 47%|████▋     | 47/100 [00:14<00:15,  3.35it/s]zhe:W_max:27.9277,W_min:-32.5808\n",
      " 48%|████▊     | 48/100 [00:14<00:15,  3.31it/s]zhe:W_max:32.4587,W_min:-32.4469\n",
      " 49%|████▉     | 49/100 [00:14<00:15,  3.35it/s]zhe:W_max:32.0666,W_min:-32.6609\n",
      " 50%|█████     | 50/100 [00:15<00:14,  3.38it/s]zhe:W_max:30.1309,W_min:-34.5734\n",
      " 51%|█████     | 51/100 [00:15<00:14,  3.34it/s]zhe:W_max:30.7968,W_min:-33.4770\n",
      " 52%|█████▏    | 52/100 [00:15<00:14,  3.41it/s]zhe:W_max:33.7944,W_min:-31.5077\n",
      " 53%|█████▎    | 53/100 [00:15<00:13,  3.38it/s]zhe:W_max:30.6361,W_min:-29.1213\n",
      " 54%|█████▍    | 54/100 [00:16<00:13,  3.30it/s]zhe:W_max:31.9685,W_min:-34.8371\n",
      " 55%|█████▌    | 55/100 [00:16<00:13,  3.30it/s]zhe:W_max:37.4251,W_min:-40.2999\n",
      " 56%|█████▌    | 56/100 [00:16<00:13,  3.25it/s]zhe:W_max:32.9039,W_min:-37.0767\n",
      " 57%|█████▋    | 57/100 [00:17<00:13,  3.26it/s]zhe:W_max:33.6466,W_min:-31.0607\n",
      " 58%|█████▊    | 58/100 [00:17<00:12,  3.28it/s]zhe:W_max:35.8726,W_min:-32.8128\n",
      " 59%|█████▉    | 59/100 [00:17<00:12,  3.24it/s]zhe:W_max:34.1385,W_min:-36.6074\n",
      " 60%|██████    | 60/100 [00:18<00:12,  3.26it/s]zhe:W_max:30.8478,W_min:-33.0592\n",
      " 61%|██████    | 61/100 [00:18<00:11,  3.28it/s]zhe:W_max:35.0194,W_min:-35.1653\n",
      " 62%|██████▏   | 62/100 [00:18<00:11,  3.24it/s]zhe:W_max:28.2895,W_min:-30.7301\n",
      " 63%|██████▎   | 63/100 [00:19<00:11,  3.29it/s]zhe:W_max:35.1925,W_min:-33.9430\n",
      " 64%|██████▍   | 64/100 [00:19<00:10,  3.31it/s]zhe:W_max:31.5566,W_min:-32.3116\n",
      " 65%|██████▌   | 65/100 [00:19<00:10,  3.31it/s]zhe:W_max:31.3658,W_min:-29.7974\n",
      " 66%|██████▌   | 66/100 [00:19<00:10,  3.36it/s]zhe:W_max:37.0557,W_min:-32.0496\n",
      " 67%|██████▋   | 67/100 [00:20<00:09,  3.32it/s]zhe:W_max:33.8923,W_min:-40.0059\n",
      " 68%|██████▊   | 68/100 [00:20<00:09,  3.30it/s]zhe:W_max:32.2731,W_min:-29.3185\n",
      " 69%|██████▉   | 69/100 [00:20<00:09,  3.32it/s]zhe:W_max:30.7106,W_min:-28.0308\n",
      " 70%|███████   | 70/100 [00:21<00:09,  3.29it/s]zhe:W_max:27.9809,W_min:-32.8809\n",
      " 71%|███████   | 71/100 [00:21<00:08,  3.29it/s]zhe:W_max:30.9734,W_min:-29.8300\n",
      " 72%|███████▏  | 72/100 [00:21<00:08,  3.30it/s]zhe:W_max:31.6454,W_min:-29.6242\n",
      " 73%|███████▎  | 73/100 [00:22<00:08,  3.26it/s]zhe:W_max:27.4315,W_min:-35.2723\n",
      " 74%|███████▍  | 74/100 [00:22<00:08,  3.21it/s]zhe:W_max:28.0881,W_min:-34.5641\n",
      " 75%|███████▌  | 75/100 [00:22<00:07,  3.21it/s]zhe:W_max:35.1236,W_min:-30.4020\n",
      " 76%|███████▌  | 76/100 [00:22<00:07,  3.25it/s]zhe:W_max:34.2891,W_min:-27.4272\n",
      " 77%|███████▋  | 77/100 [00:23<00:07,  3.19it/s]zhe:W_max:29.6074,W_min:-29.7380\n",
      " 78%|███████▊  | 78/100 [00:23<00:06,  3.25it/s]zhe:W_max:33.2517,W_min:-33.1660\n",
      " 79%|███████▉  | 79/100 [00:23<00:06,  3.35it/s]zhe:W_max:27.9248,W_min:-29.0295\n",
      " 80%|████████  | 80/100 [00:24<00:05,  3.34it/s]zhe:W_max:33.3075,W_min:-31.9175\n",
      " 81%|████████  | 81/100 [00:24<00:05,  3.27it/s]zhe:W_max:32.5180,W_min:-30.1433\n",
      " 82%|████████▏ | 82/100 [00:24<00:05,  3.28it/s]zhe:W_max:32.8345,W_min:-42.4126\n",
      " 83%|████████▎ | 83/100 [00:25<00:05,  3.29it/s]zhe:W_max:28.9418,W_min:-29.6378\n",
      " 84%|████████▍ | 84/100 [00:25<00:04,  3.33it/s]zhe:W_max:35.1784,W_min:-25.3474\n",
      " 85%|████████▌ | 85/100 [00:25<00:04,  3.32it/s]zhe:W_max:31.9420,W_min:-26.3846\n",
      " 86%|████████▌ | 86/100 [00:26<00:04,  3.31it/s]zhe:W_max:39.4659,W_min:-31.6537\n",
      " 87%|████████▋ | 87/100 [00:26<00:03,  3.26it/s]zhe:W_max:31.1202,W_min:-27.7325\n",
      " 88%|████████▊ | 88/100 [00:26<00:03,  3.25it/s]zhe:W_max:32.3563,W_min:-33.5939\n",
      " 89%|████████▉ | 89/100 [00:26<00:03,  3.28it/s]zhe:W_max:30.5050,W_min:-33.3936\n",
      " 90%|█████████ | 90/100 [00:27<00:03,  3.29it/s]zhe:W_max:34.3488,W_min:-40.2428\n",
      " 91%|█████████ | 91/100 [00:27<00:02,  3.34it/s]zhe:W_max:30.9996,W_min:-42.1313\n",
      " 92%|█████████▏| 92/100 [00:27<00:02,  3.39it/s]zhe:W_max:27.4320,W_min:-37.3216\n",
      " 93%|█████████▎| 93/100 [00:28<00:02,  3.35it/s]zhe:W_max:28.7842,W_min:-30.1832\n",
      " 94%|█████████▍| 94/100 [00:28<00:01,  3.30it/s]zhe:W_max:32.6940,W_min:-36.6418\n",
      " 95%|█████████▌| 95/100 [00:28<00:01,  3.40it/s]zhe:W_max:29.0993,W_min:-32.6503\n",
      " 96%|█████████▌| 96/100 [00:29<00:01,  3.39it/s]zhe:W_max:29.4302,W_min:-33.9090\n",
      " 97%|█████████▋| 97/100 [00:29<00:00,  3.36it/s]zhe:W_max:36.3280,W_min:-26.6909\n",
      " 98%|█████████▊| 98/100 [00:29<00:00,  3.35it/s]zhe:W_max:39.6630,W_min:-29.0360\n",
      " 99%|█████████▉| 99/100 [00:29<00:00,  3.36it/s]zhe:W_max:30.6467,W_min:-36.8360\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.31it/s]\n",
      "for Gibbs-zhe: accuracy is 0.409091, auc is 0.445833\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]ours:W_max:27.7743,W_min:-36.7325\n",
      "\n",
      " running test-auc = 0.51667\n",
      "running train-auc = 0.81833\n",
      "\n",
      "  1%|          | 1/100 [00:00<00:30,  3.22it/s]ours:W_max:30.3340,W_min:-28.8221\n",
      "  2%|▏         | 2/100 [00:00<00:30,  3.19it/s]ours:W_max:31.5374,W_min:-26.0340\n",
      "  3%|▎         | 3/100 [00:00<00:30,  3.18it/s]ours:W_max:31.8144,W_min:-26.8015\n",
      "  4%|▍         | 4/100 [00:01<00:29,  3.21it/s]ours:W_max:27.1016,W_min:-31.0848\n",
      "  5%|▌         | 5/100 [00:01<00:29,  3.20it/s]ours:W_max:27.5814,W_min:-32.7874\n",
      "  6%|▌         | 6/100 [00:01<00:29,  3.24it/s]ours:W_max:28.4188,W_min:-28.5106\n",
      "  7%|▋         | 7/100 [00:02<00:28,  3.22it/s]ours:W_max:26.7198,W_min:-34.3372\n",
      "  8%|▊         | 8/100 [00:02<00:28,  3.20it/s]ours:W_max:31.7445,W_min:-33.4323\n",
      "  9%|▉         | 9/100 [00:02<00:28,  3.21it/s]ours:W_max:33.4554,W_min:-33.9298\n",
      " 10%|█         | 10/100 [00:03<00:27,  3.23it/s]ours:W_max:32.7750,W_min:-33.4269\n",
      "\n",
      " running test-auc = 0.54583\n",
      "running train-auc = 0.97667\n",
      "\n",
      " 11%|█         | 11/100 [00:03<00:28,  3.17it/s]ours:W_max:29.8192,W_min:-32.6916\n",
      " 12%|█▏        | 12/100 [00:03<00:28,  3.09it/s]ours:W_max:28.5100,W_min:-28.3011\n",
      " 13%|█▎        | 13/100 [00:04<00:28,  3.09it/s]ours:W_max:36.6972,W_min:-30.3790\n",
      " 14%|█▍        | 14/100 [00:04<00:27,  3.08it/s]ours:W_max:32.0791,W_min:-32.0056\n",
      " 15%|█▌        | 15/100 [00:04<00:27,  3.05it/s]ours:W_max:32.8386,W_min:-33.2936\n",
      " 16%|█▌        | 16/100 [00:05<00:27,  3.06it/s]ours:W_max:32.0649,W_min:-27.5308\n",
      " 17%|█▋        | 17/100 [00:05<00:27,  3.06it/s]ours:W_max:34.0562,W_min:-35.7193\n",
      " 18%|█▊        | 18/100 [00:05<00:26,  3.05it/s]ours:W_max:36.5585,W_min:-28.5589\n",
      " 19%|█▉        | 19/100 [00:06<00:26,  3.07it/s]ours:W_max:38.9515,W_min:-33.5917\n",
      " 20%|██        | 20/100 [00:06<00:26,  3.04it/s]ours:W_max:33.9732,W_min:-34.8750\n",
      "\n",
      " running test-auc = 0.45417\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 21%|██        | 21/100 [00:06<00:26,  3.02it/s]ours:W_max:35.3801,W_min:-33.5505\n",
      " 22%|██▏       | 22/100 [00:07<00:25,  3.02it/s]ours:W_max:36.7607,W_min:-27.0372\n",
      " 23%|██▎       | 23/100 [00:07<00:25,  3.03it/s]ours:W_max:28.7577,W_min:-29.5635\n",
      " 24%|██▍       | 24/100 [00:07<00:25,  3.04it/s]ours:W_max:34.5337,W_min:-30.7088\n",
      " 25%|██▌       | 25/100 [00:08<00:24,  3.06it/s]ours:W_max:32.0966,W_min:-33.7668\n",
      " 26%|██▌       | 26/100 [00:08<00:23,  3.09it/s]ours:W_max:35.2058,W_min:-26.9559\n",
      " 27%|██▋       | 27/100 [00:08<00:23,  3.06it/s]ours:W_max:36.7733,W_min:-34.5120\n",
      " 28%|██▊       | 28/100 [00:09<00:23,  3.09it/s]ours:W_max:38.3680,W_min:-31.2455\n",
      " 29%|██▉       | 29/100 [00:09<00:23,  3.06it/s]ours:W_max:25.0544,W_min:-29.3360\n",
      " 30%|███       | 30/100 [00:09<00:22,  3.09it/s]ours:W_max:29.0210,W_min:-33.5163\n",
      "\n",
      " running test-auc = 0.42083\n",
      "running train-auc = 0.98833\n",
      "\n",
      " 31%|███       | 31/100 [00:10<00:22,  3.07it/s]ours:W_max:34.3899,W_min:-32.3203\n",
      " 32%|███▏      | 32/100 [00:10<00:22,  3.05it/s]ours:W_max:29.4801,W_min:-33.5200\n",
      " 33%|███▎      | 33/100 [00:10<00:21,  3.09it/s]ours:W_max:31.5250,W_min:-30.1234\n",
      " 34%|███▍      | 34/100 [00:10<00:21,  3.05it/s]ours:W_max:31.7943,W_min:-26.8756\n",
      " 35%|███▌      | 35/100 [00:11<00:21,  2.96it/s]ours:W_max:31.2978,W_min:-30.5838\n",
      " 36%|███▌      | 36/100 [00:11<00:21,  3.01it/s]ours:W_max:28.1367,W_min:-32.7500\n",
      " 37%|███▋      | 37/100 [00:12<00:21,  2.98it/s]ours:W_max:31.0375,W_min:-45.2822\n",
      " 38%|███▊      | 38/100 [00:12<00:20,  2.99it/s]ours:W_max:33.3602,W_min:-35.0674\n",
      " 39%|███▉      | 39/100 [00:12<00:20,  3.01it/s]ours:W_max:35.3153,W_min:-31.1464\n",
      " 40%|████      | 40/100 [00:12<00:19,  3.05it/s]ours:W_max:29.8275,W_min:-36.6186\n",
      "\n",
      " running test-auc = 0.60417\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 41%|████      | 41/100 [00:13<00:19,  3.05it/s]ours:W_max:38.9131,W_min:-30.7348\n",
      " 42%|████▏     | 42/100 [00:13<00:18,  3.12it/s]ours:W_max:29.8738,W_min:-28.7245\n",
      " 43%|████▎     | 43/100 [00:13<00:18,  3.05it/s]ours:W_max:33.2659,W_min:-29.8333\n",
      " 44%|████▍     | 44/100 [00:14<00:17,  3.12it/s]ours:W_max:32.3110,W_min:-35.1142\n",
      " 45%|████▌     | 45/100 [00:14<00:17,  3.10it/s]ours:W_max:29.7612,W_min:-29.8131\n",
      " 46%|████▌     | 46/100 [00:14<00:17,  3.10it/s]ours:W_max:36.3747,W_min:-32.3099\n",
      " 47%|████▋     | 47/100 [00:15<00:17,  3.11it/s]ours:W_max:30.0970,W_min:-30.3842\n",
      " 48%|████▊     | 48/100 [00:15<00:16,  3.10it/s]ours:W_max:29.6884,W_min:-34.4695\n",
      " 49%|████▉     | 49/100 [00:15<00:16,  3.08it/s]ours:W_max:30.2350,W_min:-28.6497\n",
      " 50%|█████     | 50/100 [00:16<00:16,  3.08it/s]ours:W_max:41.9376,W_min:-28.3609\n",
      "\n",
      " running test-auc = 0.76667\n",
      "running train-auc = 0.99333\n",
      "\n",
      " 51%|█████     | 51/100 [00:16<00:15,  3.07it/s]ours:W_max:28.9054,W_min:-38.0775\n",
      " 52%|█████▏    | 52/100 [00:16<00:15,  3.03it/s]ours:W_max:35.4348,W_min:-28.1279\n",
      " 53%|█████▎    | 53/100 [00:17<00:15,  3.06it/s]ours:W_max:33.4703,W_min:-37.2002\n",
      " 54%|█████▍    | 54/100 [00:17<00:15,  3.05it/s]ours:W_max:35.8683,W_min:-30.8495\n",
      " 55%|█████▌    | 55/100 [00:17<00:14,  3.05it/s]ours:W_max:36.0897,W_min:-31.2190\n",
      " 56%|█████▌    | 56/100 [00:18<00:14,  3.10it/s]ours:W_max:32.6142,W_min:-39.7641\n",
      " 57%|█████▋    | 57/100 [00:18<00:13,  3.13it/s]ours:W_max:31.8848,W_min:-27.9809\n",
      " 58%|█████▊    | 58/100 [00:18<00:13,  3.12it/s]ours:W_max:28.3443,W_min:-27.7526\n",
      " 59%|█████▉    | 59/100 [00:19<00:13,  3.09it/s]ours:W_max:29.8802,W_min:-33.2663\n",
      " 60%|██████    | 60/100 [00:19<00:12,  3.14it/s]ours:W_max:31.1066,W_min:-28.4005\n",
      "\n",
      " running test-auc = 0.55833\n",
      "running train-auc = 0.99667\n",
      "\n",
      " 61%|██████    | 61/100 [00:19<00:12,  3.14it/s]ours:W_max:33.2781,W_min:-30.8438\n",
      " 62%|██████▏   | 62/100 [00:20<00:12,  3.09it/s]ours:W_max:34.2641,W_min:-37.2284\n",
      " 63%|██████▎   | 63/100 [00:20<00:11,  3.12it/s]ours:W_max:31.3363,W_min:-34.5508\n",
      " 64%|██████▍   | 64/100 [00:20<00:11,  3.03it/s]ours:W_max:33.0693,W_min:-32.5485\n",
      " 65%|██████▌   | 65/100 [00:21<00:11,  3.05it/s]ours:W_max:34.8306,W_min:-34.9664\n",
      " 66%|██████▌   | 66/100 [00:21<00:11,  3.06it/s]ours:W_max:33.7572,W_min:-35.0026\n",
      " 67%|██████▋   | 67/100 [00:21<00:10,  3.01it/s]ours:W_max:27.7884,W_min:-33.2962\n",
      " 68%|██████▊   | 68/100 [00:22<00:10,  2.96it/s]ours:W_max:26.5134,W_min:-30.7076\n",
      " 69%|██████▉   | 69/100 [00:22<00:10,  2.98it/s]ours:W_max:30.9987,W_min:-35.4405\n",
      " 70%|███████   | 70/100 [00:22<00:10,  3.00it/s]ours:W_max:29.4718,W_min:-29.9147\n",
      "\n",
      " running test-auc = 0.50000\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 71%|███████   | 71/100 [00:23<00:09,  3.02it/s]ours:W_max:30.9600,W_min:-30.7784\n",
      " 72%|███████▏  | 72/100 [00:23<00:09,  3.05it/s]ours:W_max:31.1962,W_min:-25.7913\n",
      " 73%|███████▎  | 73/100 [00:23<00:08,  3.02it/s]ours:W_max:33.4953,W_min:-30.9310\n",
      " 74%|███████▍  | 74/100 [00:24<00:08,  3.01it/s]ours:W_max:30.7728,W_min:-33.3180\n",
      " 75%|███████▌  | 75/100 [00:24<00:08,  3.02it/s]ours:W_max:30.5619,W_min:-37.2352\n",
      " 76%|███████▌  | 76/100 [00:24<00:08,  2.99it/s]ours:W_max:31.9087,W_min:-27.9212\n",
      " 77%|███████▋  | 77/100 [00:25<00:07,  2.99it/s]ours:W_max:36.9750,W_min:-30.2860\n",
      " 78%|███████▊  | 78/100 [00:25<00:07,  3.03it/s]ours:W_max:32.6159,W_min:-35.2834\n",
      " 79%|███████▉  | 79/100 [00:25<00:06,  3.07it/s]ours:W_max:34.3159,W_min:-35.1662\n",
      " 80%|████████  | 80/100 [00:26<00:06,  3.04it/s]ours:W_max:32.8516,W_min:-51.5331\n",
      "\n",
      " running test-auc = 0.44167\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 81%|████████  | 81/100 [00:26<00:06,  3.02it/s]ours:W_max:31.5003,W_min:-30.4699\n",
      " 82%|████████▏ | 82/100 [00:26<00:05,  3.06it/s]ours:W_max:29.0455,W_min:-30.4483\n",
      " 83%|████████▎ | 83/100 [00:27<00:05,  3.05it/s]ours:W_max:29.0095,W_min:-35.4868\n",
      " 84%|████████▍ | 84/100 [00:27<00:05,  3.02it/s]ours:W_max:32.3813,W_min:-32.6890\n",
      " 85%|████████▌ | 85/100 [00:27<00:04,  3.05it/s]ours:W_max:30.3376,W_min:-31.8040\n",
      " 86%|████████▌ | 86/100 [00:28<00:04,  3.06it/s]ours:W_max:32.3781,W_min:-31.6465\n",
      " 87%|████████▋ | 87/100 [00:28<00:04,  3.08it/s]ours:W_max:33.4966,W_min:-38.4378\n",
      " 88%|████████▊ | 88/100 [00:28<00:03,  3.09it/s]ours:W_max:31.0623,W_min:-31.2490\n",
      " 89%|████████▉ | 89/100 [00:28<00:03,  3.09it/s]ours:W_max:34.6617,W_min:-34.8417\n",
      " 90%|█████████ | 90/100 [00:29<00:03,  3.07it/s]ours:W_max:28.9633,W_min:-33.4564\n",
      "\n",
      " running test-auc = 0.53333\n",
      "running train-auc = 0.99667\n",
      "\n",
      " 91%|█████████ | 91/100 [00:29<00:02,  3.06it/s]ours:W_max:31.7353,W_min:-30.2222\n",
      " 92%|█████████▏| 92/100 [00:29<00:02,  3.02it/s]ours:W_max:29.7922,W_min:-33.2796\n",
      " 93%|█████████▎| 93/100 [00:30<00:02,  3.09it/s]ours:W_max:35.8580,W_min:-41.7414\n",
      " 94%|█████████▍| 94/100 [00:30<00:01,  3.09it/s]ours:W_max:29.0230,W_min:-38.7265\n",
      " 95%|█████████▌| 95/100 [00:30<00:01,  3.11it/s]ours:W_max:36.6031,W_min:-31.7346\n",
      " 96%|█████████▌| 96/100 [00:31<00:01,  3.08it/s]ours:W_max:32.0646,W_min:-36.2061\n",
      " 97%|█████████▋| 97/100 [00:31<00:00,  3.10it/s]ours:W_max:26.0488,W_min:-34.0825\n",
      " 98%|█████████▊| 98/100 [00:31<00:00,  3.11it/s]ours:W_max:32.6216,W_min:-28.0734\n",
      " 99%|█████████▉| 99/100 [00:32<00:00,  3.09it/s]ours:W_max:34.9307,W_min:-32.5069\n",
      "100%|██████████| 100/100 [00:32<00:00,  3.07it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.60833, acr = 0.59091\n",
      "\n",
      "\n",
      " final test auc_full = 0.62500, acr_full = 0.59091\n",
      "\n",
      "\n",
      "ours_acr_mean: 0.5455,ours_acr_std: 0.0371 \n",
      "gibbs_zhe_acr_mean: 0.4848,gibbs_zhe_acr_std: 0.0567 \n",
      "lr_acr_mean: 0.6212,lr_acr_std: 0.1134 \n",
      "esnet_acr_mean: 0.6212,esnet_acr_std: 0.1134 \n",
      "rf_acr_mean: 0.5152,rf_acr_mean: 0.1193 \n",
      "just-mode_acr_mean: 0.4394,mode_acr_std: 0.0567 \n",
      "just-random_acr_mean: 0.5303,just-random_acr_std: 0.0567 \n",
      "\n",
      "ours_AUC_mean: 0.6690,ours_AUC_std: 0.0596 \n",
      "gibbs_zhe_AUC_mean: 0.4738,gibbs_zhe_AUC_std: 0.0240 \n",
      "lr_AUC_mean: 0.7283,lr_AUC_std: 0.0466 \n",
      "esnet_AUC_mean: 0.7164,esnet_AUC_mean: 0.0486 \n",
      "rf_AUC_mean: 0.5383,rf_AUC_std: 0.1324 \n"
     ]
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1]),\n",
       " array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1]),\n",
       " array([1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1]),\n",
       " array([0, 1, 1, 1, 1, 1]),\n",
       " array([1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1]),\n",
       " array([1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# non-sparse feature of the last samples\n",
    "len(np.where(np.concatenate((model.W))>0)[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "581"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# non-sparse feature of the mean of all samples (no truncated)\n",
    "N_collect = len(model.W_collect)\n",
    "if N_collect > 0:\n",
    "    W_samples = np.zeros((N_collect,model.N_feature))\n",
    "    for i in range(N_collect):\n",
    "        W_samples[i,:-1] = np.concatenate((model.W_collect[i]))\n",
    "        W_samples[i,-1] = model.b_collect[i]\n",
    "\n",
    "    W_avg = np.mean(W_samples,axis=0).reshape(-1,1)\n",
    "len(np.where(np.abs(W_avg)>0.01)[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "N_collect = len(model.W_collect)\n",
    "if N_collect > 0:\n",
    "    # W_samples = np.zeros((N_collect,model.N_feature))\n",
    "    W_samples = np.zeros((model.N_feature,N_collect))\n",
    "    for i in range(N_collect):\n",
    "        W_samples[:-1,i] = np.concatenate((model.W_collect[i]))\n",
    "        W_samples[-1,i] = model.b_collect[i]\n",
    "\n",
    "    # W_avg = np.mean(W_samples,axis=0).reshape(-1,1)\n",
    "W_samples.shape    "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1140, 30)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(l1_ratios =[.001,.01, .1, .5, .7, .9, .95, .99, 1],penalty='elasticnet',solver='saga',fit_intercept=False).fit(data_dict['X_tr'],data_dict['y_tr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([21.5443469])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "clf.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# non-sparse feature of the Gibbs_zhe\n",
    "dict_gibbs_zhe['model'][1][:, -10:]."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1140, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  }
 ]
}