{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd06ee6abd9b0d44c31108424068010123bef58fe758ec0be567da6fa0551538e82",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cb03ba31accc2f25e369d1aa3152af3569125a23627c23321dfacf015e0ff229"
   }
  },
  "interpreter": {
   "hash": "cb03ba31accc2f25e369d1aa3152af3569125a23627c23321dfacf015e0ff229"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy\n",
    "# import xlrd \n",
    "import sklearn\n",
    "\n",
    "from Gibbs_model_probit import Gibbs_sampling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import multivariate_normal\n",
    "from utils import baseline_lr,baseline_esnet,baseline_justmean\n",
    "from utils import baseline_LogitElsnet,baseline_justmode,baseline_random,baseline_LogitLR,baseline_RanForest\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import binom \n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loading (we will use the simu data first)\n",
    "\n",
    "# data_table = pd.read_csv('../data/processed/all_feature_p2_lip_specie.csv')\n",
    "data_table = pd.read_csv('../data/processed/all_feature_p2_lip_specie.csv')\n",
    "# data_table = pd.read_csv('../data/processed/all_feature_p1_lip_specie.csv')\n",
    "target = 'gap_surv_time_class'\n",
    "# target = '1= death; 0=alive'\n",
    "\n",
    "\n",
    "\n",
    "# normalization\n",
    "\n",
    "# min-max\n",
    "df = data_table[target]\n",
    "\n",
    "\n",
    "# check nan\n",
    "data_table[target].isnull().values.any()\n",
    "data_table.fillna(data_table.mean(), inplace=True) # fill nan with column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "early dead:48 ,middle:51,long survive:45 \n"
     ]
    }
   ],
   "source": [
    "gene_feature = data_table.iloc[:,1:775]\n",
    "# Y = df.values\n",
    "\n",
    "y = data_table[target].values\n",
    "print('early dead:%d ,middle:%d,long survive:%d '%((y==0).sum(),(y==2).sum(),(y==1).sum()))\n",
    "Y = np.where(y>0,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     lip_Sph(d16:1)_27  lip_Sph(d18:1)_27  lip_Sph(d18:2)_27  \\\n",
       "0             1.508517          -1.142981          -0.370896   \n",
       "1             0.662510          -0.169350           0.276919   \n",
       "2             0.382097           0.034175          -0.873632   \n",
       "3            -0.505375          -0.856168          -0.309318   \n",
       "4            -0.973029          -1.221808           0.084801   \n",
       "..                 ...                ...                ...   \n",
       "139           0.836448           0.078052          -0.680276   \n",
       "140           0.840385           0.370737          -0.795050   \n",
       "141           0.958798          -0.450028           0.631433   \n",
       "142          -0.486482          -0.715181          -0.765942   \n",
       "143           0.517969           0.222055           1.282967   \n",
       "\n",
       "     lip_S1P(d16:1)_6  lip_S1P(d18:0)_6  lip_S1P(d18:1)_6  lip_S1P(d18:2)_6  \\\n",
       "0            1.161898          1.393186          2.439157          3.142364   \n",
       "1           -0.420598          0.890299          1.172982          0.270768   \n",
       "2            0.263159          0.442293         -0.246913         -0.094627   \n",
       "3           -0.876899          0.129396         -0.332582         -0.197593   \n",
       "4            1.401033          0.822457          0.869329          0.284968   \n",
       "..                ...               ...               ...               ...   \n",
       "139         -0.222538         -1.176785         -1.582970         -1.824283   \n",
       "140         -0.544087         -0.086341         -0.931198         -1.267266   \n",
       "141          1.413756         -0.018409         -0.517109          0.988292   \n",
       "142         -1.725063         -1.572528         -1.536034         -1.350552   \n",
       "143         -0.977544         -0.668532         -0.798935         -1.308465   \n",
       "\n",
       "     lip_Cer(d18:0/16:0)_3  lip_Cer(d18:0/18:0)_3  lip_Cer(d18:0/20:0)_3  ...  \\\n",
       "0                 2.807684               1.906719               1.964852  ...   \n",
       "1                 1.235381              -0.120123               0.356863  ...   \n",
       "2                 0.488499              -1.068774              -0.575590  ...   \n",
       "3                 1.159958               1.085276               0.743728  ...   \n",
       "4                 0.232719               0.030465              -0.985736  ...   \n",
       "..                     ...                    ...                    ...  ...   \n",
       "139               0.018785               1.688315               0.036113  ...   \n",
       "140              -0.992808               0.144297              -0.843535  ...   \n",
       "141              -1.232623               0.418946              -0.962435  ...   \n",
       "142              -1.357425              -1.695333              -0.907401  ...   \n",
       "143               0.653114               0.069228               0.664920  ...   \n",
       "\n",
       "     lip_TG(O-54:4) [NL-18:2]_13  lip_Ubiquinone_24  lip_CE(18:2) [+OH]_15  \\\n",
       "0                       0.036007           0.359336              -0.383320   \n",
       "1                      -0.656573          -0.557216               4.805999   \n",
       "2                       2.257551          -0.525753              -0.165085   \n",
       "3                       1.085764           0.103507              -0.345465   \n",
       "4                      -1.369647          -1.081999               5.153219   \n",
       "..                           ...                ...                    ...   \n",
       "139                    -0.919553          -0.499796               0.454661   \n",
       "140                    -0.504697          -0.009101               0.544398   \n",
       "141                    -0.217015           0.367528               0.289700   \n",
       "142                    -0.470357          -0.925047               0.599696   \n",
       "143                     1.043798          -0.149132               0.092477   \n",
       "\n",
       "     lip_CE(20:4) [+OH]_15  lip_CE(22:6) [+OH]_15  lip_LPC(18:2) [+OH]_15  \\\n",
       "0                -0.018562              -0.205400               -0.186943   \n",
       "1                 2.009266               2.187153               11.290922   \n",
       "2                -0.231418              -0.152552               -0.118845   \n",
       "3                -0.515169              -0.190832               -0.202784   \n",
       "4                 7.136091               8.406937                4.119153   \n",
       "..                     ...                    ...                     ...   \n",
       "139               0.274775              -0.177827               -0.122108   \n",
       "140              -0.174427              -0.209744               -0.171430   \n",
       "141               0.035265              -0.179299               -0.172936   \n",
       "142              -0.066377              -0.217330               -0.155953   \n",
       "143               0.002583              -0.135165               -0.168080   \n",
       "\n",
       "     lip_LPC(20:4) [+OH]_15  lip_LPC(22:6) [+OH]_15  lip_PC(34:2) [+OH]_15  \\\n",
       "0                 -0.090134               -0.100703              -0.293183   \n",
       "1                  7.801393                8.273906              10.974755   \n",
       "2                 -0.172070               -0.142195              -0.062192   \n",
       "3                 -0.200526               -0.244971              -0.218103   \n",
       "4                  6.956574                8.631769               6.050933   \n",
       "..                      ...                     ...                    ...   \n",
       "139               -0.138097               -0.105071              -0.128006   \n",
       "140               -0.183107               -0.169494              -0.137458   \n",
       "141               -0.163742               -0.136950              -0.152169   \n",
       "142               -0.172993               -0.199861              -0.191259   \n",
       "143               -0.170067               -0.114229               0.082395   \n",
       "\n",
       "     lip_PC(36:4) [+OH]_15  \n",
       "0                 1.595553  \n",
       "1                 2.317701  \n",
       "2                 0.139186  \n",
       "3                -1.082805  \n",
       "4                 3.055967  \n",
       "..                     ...  \n",
       "139              -0.260284  \n",
       "140              -0.804552  \n",
       "141              -0.558493  \n",
       "142              -0.745833  \n",
       "143               0.530311  \n",
       "\n",
       "[144 rows x 774 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lip_Sph(d16:1)_27</th>\n      <th>lip_Sph(d18:1)_27</th>\n      <th>lip_Sph(d18:2)_27</th>\n      <th>lip_S1P(d16:1)_6</th>\n      <th>lip_S1P(d18:0)_6</th>\n      <th>lip_S1P(d18:1)_6</th>\n      <th>lip_S1P(d18:2)_6</th>\n      <th>lip_Cer(d18:0/16:0)_3</th>\n      <th>lip_Cer(d18:0/18:0)_3</th>\n      <th>lip_Cer(d18:0/20:0)_3</th>\n      <th>...</th>\n      <th>lip_TG(O-54:4) [NL-18:2]_13</th>\n      <th>lip_Ubiquinone_24</th>\n      <th>lip_CE(18:2) [+OH]_15</th>\n      <th>lip_CE(20:4) [+OH]_15</th>\n      <th>lip_CE(22:6) [+OH]_15</th>\n      <th>lip_LPC(18:2) [+OH]_15</th>\n      <th>lip_LPC(20:4) [+OH]_15</th>\n      <th>lip_LPC(22:6) [+OH]_15</th>\n      <th>lip_PC(34:2) [+OH]_15</th>\n      <th>lip_PC(36:4) [+OH]_15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.508517</td>\n      <td>-1.142981</td>\n      <td>-0.370896</td>\n      <td>1.161898</td>\n      <td>1.393186</td>\n      <td>2.439157</td>\n      <td>3.142364</td>\n      <td>2.807684</td>\n      <td>1.906719</td>\n      <td>1.964852</td>\n      <td>...</td>\n      <td>0.036007</td>\n      <td>0.359336</td>\n      <td>-0.383320</td>\n      <td>-0.018562</td>\n      <td>-0.205400</td>\n      <td>-0.186943</td>\n      <td>-0.090134</td>\n      <td>-0.100703</td>\n      <td>-0.293183</td>\n      <td>1.595553</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.662510</td>\n      <td>-0.169350</td>\n      <td>0.276919</td>\n      <td>-0.420598</td>\n      <td>0.890299</td>\n      <td>1.172982</td>\n      <td>0.270768</td>\n      <td>1.235381</td>\n      <td>-0.120123</td>\n      <td>0.356863</td>\n      <td>...</td>\n      <td>-0.656573</td>\n      <td>-0.557216</td>\n      <td>4.805999</td>\n      <td>2.009266</td>\n      <td>2.187153</td>\n      <td>11.290922</td>\n      <td>7.801393</td>\n      <td>8.273906</td>\n      <td>10.974755</td>\n      <td>2.317701</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.382097</td>\n      <td>0.034175</td>\n      <td>-0.873632</td>\n      <td>0.263159</td>\n      <td>0.442293</td>\n      <td>-0.246913</td>\n      <td>-0.094627</td>\n      <td>0.488499</td>\n      <td>-1.068774</td>\n      <td>-0.575590</td>\n      <td>...</td>\n      <td>2.257551</td>\n      <td>-0.525753</td>\n      <td>-0.165085</td>\n      <td>-0.231418</td>\n      <td>-0.152552</td>\n      <td>-0.118845</td>\n      <td>-0.172070</td>\n      <td>-0.142195</td>\n      <td>-0.062192</td>\n      <td>0.139186</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>-0.505375</td>\n      <td>-0.856168</td>\n      <td>-0.309318</td>\n      <td>-0.876899</td>\n      <td>0.129396</td>\n      <td>-0.332582</td>\n      <td>-0.197593</td>\n      <td>1.159958</td>\n      <td>1.085276</td>\n      <td>0.743728</td>\n      <td>...</td>\n      <td>1.085764</td>\n      <td>0.103507</td>\n      <td>-0.345465</td>\n      <td>-0.515169</td>\n      <td>-0.190832</td>\n      <td>-0.202784</td>\n      <td>-0.200526</td>\n      <td>-0.244971</td>\n      <td>-0.218103</td>\n      <td>-1.082805</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>-0.973029</td>\n      <td>-1.221808</td>\n      <td>0.084801</td>\n      <td>1.401033</td>\n      <td>0.822457</td>\n      <td>0.869329</td>\n      <td>0.284968</td>\n      <td>0.232719</td>\n      <td>0.030465</td>\n      <td>-0.985736</td>\n      <td>...</td>\n      <td>-1.369647</td>\n      <td>-1.081999</td>\n      <td>5.153219</td>\n      <td>7.136091</td>\n      <td>8.406937</td>\n      <td>4.119153</td>\n      <td>6.956574</td>\n      <td>8.631769</td>\n      <td>6.050933</td>\n      <td>3.055967</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>0.836448</td>\n      <td>0.078052</td>\n      <td>-0.680276</td>\n      <td>-0.222538</td>\n      <td>-1.176785</td>\n      <td>-1.582970</td>\n      <td>-1.824283</td>\n      <td>0.018785</td>\n      <td>1.688315</td>\n      <td>0.036113</td>\n      <td>...</td>\n      <td>-0.919553</td>\n      <td>-0.499796</td>\n      <td>0.454661</td>\n      <td>0.274775</td>\n      <td>-0.177827</td>\n      <td>-0.122108</td>\n      <td>-0.138097</td>\n      <td>-0.105071</td>\n      <td>-0.128006</td>\n      <td>-0.260284</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.840385</td>\n      <td>0.370737</td>\n      <td>-0.795050</td>\n      <td>-0.544087</td>\n      <td>-0.086341</td>\n      <td>-0.931198</td>\n      <td>-1.267266</td>\n      <td>-0.992808</td>\n      <td>0.144297</td>\n      <td>-0.843535</td>\n      <td>...</td>\n      <td>-0.504697</td>\n      <td>-0.009101</td>\n      <td>0.544398</td>\n      <td>-0.174427</td>\n      <td>-0.209744</td>\n      <td>-0.171430</td>\n      <td>-0.183107</td>\n      <td>-0.169494</td>\n      <td>-0.137458</td>\n      <td>-0.804552</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.958798</td>\n      <td>-0.450028</td>\n      <td>0.631433</td>\n      <td>1.413756</td>\n      <td>-0.018409</td>\n      <td>-0.517109</td>\n      <td>0.988292</td>\n      <td>-1.232623</td>\n      <td>0.418946</td>\n      <td>-0.962435</td>\n      <td>...</td>\n      <td>-0.217015</td>\n      <td>0.367528</td>\n      <td>0.289700</td>\n      <td>0.035265</td>\n      <td>-0.179299</td>\n      <td>-0.172936</td>\n      <td>-0.163742</td>\n      <td>-0.136950</td>\n      <td>-0.152169</td>\n      <td>-0.558493</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>-0.486482</td>\n      <td>-0.715181</td>\n      <td>-0.765942</td>\n      <td>-1.725063</td>\n      <td>-1.572528</td>\n      <td>-1.536034</td>\n      <td>-1.350552</td>\n      <td>-1.357425</td>\n      <td>-1.695333</td>\n      <td>-0.907401</td>\n      <td>...</td>\n      <td>-0.470357</td>\n      <td>-0.925047</td>\n      <td>0.599696</td>\n      <td>-0.066377</td>\n      <td>-0.217330</td>\n      <td>-0.155953</td>\n      <td>-0.172993</td>\n      <td>-0.199861</td>\n      <td>-0.191259</td>\n      <td>-0.745833</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>0.517969</td>\n      <td>0.222055</td>\n      <td>1.282967</td>\n      <td>-0.977544</td>\n      <td>-0.668532</td>\n      <td>-0.798935</td>\n      <td>-1.308465</td>\n      <td>0.653114</td>\n      <td>0.069228</td>\n      <td>0.664920</td>\n      <td>...</td>\n      <td>1.043798</td>\n      <td>-0.149132</td>\n      <td>0.092477</td>\n      <td>0.002583</td>\n      <td>-0.135165</td>\n      <td>-0.168080</td>\n      <td>-0.170067</td>\n      <td>-0.114229</td>\n      <td>0.082395</td>\n      <td>0.530311</td>\n    </tr>\n  </tbody>\n</table>\n<p>144 rows × 774 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "gene_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(gene_feature.columns)\n",
    "K=41 # group number, from data process notebook\n",
    "group_ind_dict = {}\n",
    "group_ind = []\n",
    "group_ind_concat = []\n",
    "for i in range(K):\n",
    "    group_ind_dict[i] = []\n",
    "\n",
    "for name in feature_names:\n",
    "    id = int(name.split('_')[-1])\n",
    "    group_ind_dict[id].append(name)\n",
    "\n",
    "for i in range(K):\n",
    "    group_ind.append(group_ind_dict[i])\n",
    "    group_ind_concat = group_ind_concat + group_ind_dict[i]\n",
    "\n",
    "# group_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(144, 775)\n"
     ]
    }
   ],
   "source": [
    "# re-arrange the features of X based on the group split order\n",
    "X_new = gene_feature[group_ind_concat].values\n",
    "\n",
    "N_sample,_ = X_new.shape\n",
    "# add all-one column at the last \n",
    "bias_col = np.ones(N_sample).reshape((N_sample,1))\n",
    "X_new = np.concatenate((X_new,bias_col),axis=1)\n",
    "\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init hyper-parameters\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "r0 = 1e-4\n",
    "r1 = 1.0\n",
    "a0 = 1.0\n",
    "b0 = 1.0\n",
    "JITTER = 1e-3\n",
    "\n",
    "INTERVAL = 50\n",
    "VALITA_INTERVAL = 200\n",
    "BURNING = 2000\n",
    "MAX_NUMBER = 5000\n",
    "\n",
    "hyper_paras = {'INTERVAL':INTERVAL, 'BURNING':BURNING,'MAX_NUMBER':MAX_NUMBER,'VALITA_INTERVAL':VALITA_INTERVAL,\n",
    "'alpha':alpha, 'beta':beta,'r0':r0,'r1':r1,'JITTER':JITTER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0.518519\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.666667, auc is 0.649091,  fpr is 0.500000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.666667, auc is 0.655455,  fpr is 0.500000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.763889, auc is 0.760455,  fpr is 0.681818\n",
      "for justmode: accuracy is 0.694444, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.500000, fpr is 0.590909\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.555556, auc is 0.586364,  fpr is 0.454545\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.541667, auc is 0.589091,  fpr is 0.409091\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.652778, auc is 0.665455,  fpr is 0.318182\n",
      "for justmode: accuracy is 0.694444, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.444444, fpr is 0.590909\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.597222, auc is 0.594618,  fpr is 0.458333\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.569444, auc is 0.576389,  fpr is 0.500000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.597222, auc is 0.598524,  fpr is 0.583333\n",
      "for justmode: accuracy is 0.666667, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.486111, fpr is 0.458333\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.638889, auc is 0.604545,  fpr is 0.409091\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.638889, auc is 0.646364,  fpr is 0.363636\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.736111, auc is 0.708636,  fpr is 0.318182\n",
      "for justmode: accuracy is 0.694444, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.500000, fpr is 0.636364\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.638889, auc is 0.685892,  fpr is 0.521739\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.569444, auc is 0.620231,  fpr is 0.608696\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.486111, auc is 0.464508,  fpr is 0.782609\n",
      "for justmode: accuracy is 0.680556, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.555556, fpr is 0.434783\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.638889, auc is 0.615313,  fpr is 0.476190\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.638889, auc is 0.614379,  fpr is 0.428571\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.666667, auc is 0.641457,  fpr is 0.761905\n",
      "for justmode: accuracy is 0.708333, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.416667, fpr is 0.619048\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.597222, auc is 0.605042,  fpr is 0.333333\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.583333, auc is 0.627451,  fpr is 0.380952\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.611111, auc is 0.517274,  fpr is 0.761905\n",
      "for justmode: accuracy is 0.708333, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.500000, fpr is 0.571429\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.569444, auc is 0.517303,  fpr is 0.608696\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.541667, auc is 0.503993,  fpr is 0.565217\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.597222, auc is 0.580745,  fpr is 0.565217\n",
      "for justmode: accuracy is 0.680556, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.486111, fpr is 0.521739\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.625000, auc is 0.619565,  fpr is 0.461538\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.625000, auc is 0.617893,  fpr is 0.384615\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.597222, auc is 0.619147,  fpr is 0.769231\n",
      "for justmode: accuracy is 0.638889, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.513889, fpr is 0.576923\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.611111, auc is 0.644965,  fpr is 0.541667\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.555556, auc is 0.602431,  fpr is 0.500000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.722222, auc is 0.687934,  fpr is 0.416667\n",
      "for justmode: accuracy is 0.666667, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.527778, fpr is 0.416667\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.694444, auc is 0.697021,  fpr is 0.400000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.611111, auc is 0.644255,  fpr is 0.480000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.777778, auc is 0.747660,  fpr is 0.520000\n",
      "for justmode: accuracy is 0.652778, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.652778, fpr is 0.360000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.555556, auc is 0.530345,  fpr is 0.476190\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.527778, auc is 0.565826,  fpr is 0.428571\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.527778, auc is 0.515406,  fpr is 0.523810\n",
      "for justmode: accuracy is 0.708333, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.625000, fpr is 0.285714\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.597222, auc is 0.601597,  fpr is 0.608696\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.569444, auc is 0.560781,  fpr is 0.608696\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.694444, auc is 0.633984,  fpr is 0.608696\n",
      "for justmode: accuracy is 0.680556, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.500000, fpr is 0.478261\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.569444, auc is 0.533279,  fpr is 0.642857\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.555556, auc is 0.546266,  fpr is 0.535714\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.611111, auc is 0.593750,  fpr is 0.678571\n",
      "for justmode: accuracy is 0.611111, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.486111, fpr is 0.607143\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.569444, auc is 0.542387,  fpr is 0.666667\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.555556, auc is 0.560494,  fpr is 0.703704\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.638889, auc is 0.619342,  fpr is 0.814815\n",
      "for justmode: accuracy is 0.625000, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.513889, fpr is 0.518519\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.638889, auc is 0.685204,  fpr is 0.315789\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.625000, auc is 0.654419,  fpr is 0.368421\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.680556, auc is 0.654419,  fpr is 0.578947\n",
      "for justmode: accuracy is 0.736111, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.500000, fpr is 0.578947\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.597222, auc is 0.609091,  fpr is 0.409091\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.583333, auc is 0.601818,  fpr is 0.409091\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.694444, auc is 0.632727,  fpr is 0.500000\n",
      "for justmode: accuracy is 0.694444, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.486111, fpr is 0.409091\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.611111, auc is 0.680568,  fpr is 0.521739\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.625000, auc is 0.663709,  fpr is 0.478261\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.611111, auc is 0.563886,  fpr is 0.782609\n",
      "for justmode: accuracy is 0.680556, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.500000, fpr is 0.521739\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.625000, auc is 0.657872,  fpr is 0.480000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.694444, auc is 0.736170,  fpr is 0.440000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.611111, auc is 0.595745,  fpr is 0.560000\n",
      "for justmode: accuracy is 0.652778, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.486111, fpr is 0.320000\n",
      "\n",
      "\n",
      "lr_acr_mean: 0.6092,lr_acr_std: 0.0536 \n",
      "esnet_acr_mean: 0.5947,esnet_acr_mean: 0.0468 \n",
      "rf_acr_mean: 0.6361,rf_acr_mean: 0.0579 \n",
      "just-mode_acr_mean: 0.6706,mode_acr_std: 0.0380 \n",
      "just-random_acr_mean: 0.5072,just-random_acr_std: 0.0446 \n",
      "lr_AUC_mean: 0.6183,lr_AUC_std: 0.0616 \n",
      "esnet_AUC_mean: 0.6091,esnet_AUC_mean: 0.0549 \n",
      "rf_AUC_mean: 0.6205,rf_AUC_std: 0.0566 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = 50\n",
    "lr_acc = np.zeros(N)\n",
    "rf_acc = np.zeros(N)\n",
    "esnet_acc = np.zeros(N)\n",
    "mode_acc = np.zeros(N)\n",
    "random_acc = np.zeros(N)\n",
    "\n",
    "lr_auc = np.zeros(N)\n",
    "rf_auc = np.zeros(N)\n",
    "esnet_auc = np.zeros(N)\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, Y.squeeze(),test_size=0.5)\n",
    "    data_dict = {'X_tr':X_train, 'y_tr':y_train, 'X_test':X_test, 'y_test':y_test}  \n",
    "    dict_lr = baseline_LogitLR(data_dict)\n",
    "    dict_els = baseline_LogitElsnet(data_dict)\n",
    "    dict_rf = baseline_RanForest(data_dict)\n",
    "    dict_mode = baseline_justmode(data_dict)\n",
    "    dict_random = baseline_random(data_dict)\n",
    "\n",
    "    lr_acc[i] = dict_lr['acr']\n",
    "    esnet_acc[i] = dict_els['acr']\n",
    "    rf_acc[i] = dict_rf['acr']\n",
    "    mode_acc[i] = dict_mode['acr']\n",
    "    random_acc[i] = dict_random['acr']\n",
    "\n",
    "    lr_auc[i] = dict_lr['auc']\n",
    "    rf_auc[i] = dict_rf['auc']\n",
    "    esnet_auc[i] = dict_els['auc']\n",
    "\n",
    "\n",
    "print('\\n\\nlr_acr_mean: %.4f,lr_acr_std: %.4f '%(lr_acc.mean(),lr_acc.std() ) )\n",
    "print('esnet_acr_mean: %.4f,esnet_acr_mean: %.4f '%(esnet_acc.mean(),esnet_acc.std() ) )\n",
    "print('rf_acr_mean: %.4f,rf_acr_mean: %.4f '%(rf_acc.mean(),rf_acc.std() ) )\n",
    "print('just-mode_acr_mean: %.4f,mode_acr_std: %.4f '%(mode_acc.mean(),mode_acc.std() ) )\n",
    "print('just-random_acr_mean: %.4f,just-random_acr_std: %.4f '%(random_acc.mean(),random_acc.std() ) )\n",
    "\n",
    "print('lr_AUC_mean: %.4f,lr_AUC_std: %.4f '%(lr_auc.mean(),lr_auc.std() ) )\n",
    "print('esnet_AUC_mean: %.4f,esnet_AUC_mean: %.4f '%(esnet_auc.mean(),esnet_auc.std() ) )\n",
    "print('rf_AUC_mean: %.4f,rf_AUC_std: %.4f '%(rf_auc.mean(),rf_auc.std() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init parameters\n",
    "# z_array_init = np.random.binomial(size=K, n=1, p= alpha)\n",
    "# s_list_init = [np.random.binomial(size=len(item), n=1, p= beta) for item in group_ind]\n",
    "# b_init = np.random.normal(loc=0.0, scale=r1,size=None)\n",
    "# tau_init = np.random.gamma(shape=alpha, scale=1.0/beta, size=None)\n",
    "# W_init = []\n",
    "\n",
    "# for i in range(K):\n",
    "#     mask1 = 1-z_array_init[i] * s_list_init[i]\n",
    "#     mask2 = z_array_init[i] * s_list_init[i]\n",
    "#     spike = np.random.normal(loc=0.0, scale=r0,size=len(s_list_init[i]))\n",
    "#     slab = np.random.normal(loc=0.0, scale=r1,size=len(s_list_init[i]))\n",
    "#     W_group = spike * mask1 + slab * mask2\n",
    "\n",
    "#     W_init.append(W_group)\n",
    "\n",
    "# init_paras = {'z':z_array_init, 's':s_list_init, 'b':b_init, 'tau':tau_init, 'W':W_init,'a0':a0,'b0':b0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init parameters with lr_result\n",
    "def get_init_paras(w_lr):\n",
    "    z_array_init = np.ones(K) #np.random.binomial(size=K, n=1, p= alpha)\n",
    "    s_list_init = [np.random.binomial(size=len(item), n=1, p= beta) for item in group_ind]\n",
    "    b_init = w_lr[-1]#np.random.normal(loc=0.0, scale=r1,size=None)\n",
    "    # tau_init = 1.0#np.random.gamma(shape=alpha, scale=1.0/beta, size=None)\n",
    "\n",
    "    W_init = []\n",
    "    offset=0\n",
    "    for i in range(K):\n",
    "        # mask1 = 1-z_array_init[i] * s_list_init[i]\n",
    "        # mask2 = z_array_init[i] * s_list_init[i]\n",
    "        # spike = np.random.normal(loc=0.0, scale=r0,size=len(s_list_init[i]))\n",
    "        # slab = np.random.normal(loc=0.0, scale=r1,size=len(s_list_init[i]))\n",
    "        # W_group = spike * mask1 + slab * mask2\n",
    "\n",
    "        \n",
    "        group_len = len(s_list_init[i])\n",
    "        W_group= w_lr[offset:offset+group_len]\n",
    "        offset = offset + group_len\n",
    "        W_init.append(W_group)\n",
    "\n",
    "    init_paras = {'z':z_array_init, 's':s_list_init, 'b':b_init,  'W':W_init,'a0':a0,'b0':b0}\n",
    "    return init_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "st-auc = 0.56105\n",
      "running train-auc = 0.99029\n",
      "\n",
      " 92%|█████████▏| 4600/5000 [10:14<00:49,  8.03it/s]\n",
      " running test-auc = 0.65895\n",
      "running train-auc = 0.99417\n",
      "\n",
      " 96%|█████████▌| 4800/5000 [10:40<00:27,  7.19it/s]\n",
      " running test-auc = 0.62737\n",
      "running train-auc = 0.99611\n",
      "\n",
      "100%|██████████| 5000/5000 [11:07<00:00,  7.49it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.54105, acr = 0.52273\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.545455, auc is 0.511983,  fpr is 0.588235\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.545455, auc is 0.570806,  fpr is 0.588235\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.636364, auc is 0.659041,  fpr is 0.647059\n",
      "for justmode: accuracy is 0.613636, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.522727, fpr is 0.352941\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.53595\n",
      "running train-auc = 1.00000\n",
      "\n",
      "  4%|▍         | 200/5000 [00:26<10:10,  7.87it/s]\n",
      " running test-auc = 0.56645\n",
      "running train-auc = 0.99532\n",
      "\n",
      "  8%|▊         | 400/5000 [00:52<10:05,  7.59it/s]\n",
      " running test-auc = 0.53377\n",
      "running train-auc = 0.99439\n",
      "\n",
      " 12%|█▏        | 600/5000 [01:18<09:29,  7.73it/s]\n",
      " running test-auc = 0.42919\n",
      "running train-auc = 0.99719\n",
      "\n",
      " 16%|█▌        | 800/5000 [01:45<10:46,  6.50it/s]\n",
      " running test-auc = 0.56754\n",
      "running train-auc = 0.99906\n",
      "\n",
      " 20%|██        | 1000/5000 [02:12<08:59,  7.42it/s]\n",
      " running test-auc = 0.54248\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 24%|██▍       | 1200/5000 [02:38<08:22,  7.56it/s]\n",
      " running test-auc = 0.56536\n",
      "running train-auc = 0.99860\n",
      "\n",
      " 28%|██▊       | 1400/5000 [03:05<07:57,  7.55it/s]\n",
      " running test-auc = 0.68083\n",
      "running train-auc = 0.99813\n",
      "\n",
      " 32%|███▏      | 1600/5000 [03:31<07:39,  7.40it/s]\n",
      " running test-auc = 0.54139\n",
      "running train-auc = 0.99439\n",
      "\n",
      " 36%|███▌      | 1800/5000 [03:57<06:51,  7.78it/s]\n",
      " running test-auc = 0.67647\n",
      "running train-auc = 0.99626\n",
      "\n",
      " 40%|████      | 2000/5000 [04:23<06:42,  7.45it/s]\n",
      " running test-auc = 0.42266\n",
      "running train-auc = 0.99766\n",
      "\n",
      " 44%|████▍     | 2200/5000 [04:49<06:09,  7.58it/s]\n",
      " running test-auc = 0.50763\n",
      "running train-auc = 0.99205\n",
      "\n",
      " 48%|████▊     | 2400/5000 [05:15<05:31,  7.84it/s]\n",
      " running test-auc = 0.37800\n",
      "running train-auc = 0.99486\n",
      "\n",
      " 52%|█████▏    | 2600/5000 [05:41<05:18,  7.54it/s]\n",
      " running test-auc = 0.64161\n",
      "running train-auc = 0.99766\n",
      "\n",
      " 56%|█████▌    | 2800/5000 [06:08<04:51,  7.54it/s]\n",
      " running test-auc = 0.56318\n",
      "running train-auc = 0.99766\n",
      "\n",
      " 60%|██████    | 3000/5000 [06:34<04:17,  7.77it/s]\n",
      " running test-auc = 0.45861\n",
      "running train-auc = 0.99579\n",
      "\n",
      " 64%|██████▍   | 3200/5000 [07:00<03:46,  7.95it/s]\n",
      " running test-auc = 0.45643\n",
      "running train-auc = 0.99392\n",
      "\n",
      " 68%|██████▊   | 3400/5000 [07:26<03:46,  7.06it/s]\n",
      " running test-auc = 0.52723\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 72%|███████▏  | 3600/5000 [07:52<03:08,  7.42it/s]\n",
      " running test-auc = 0.58606\n",
      "running train-auc = 0.99532\n",
      "\n",
      " 76%|███████▌  | 3800/5000 [08:18<02:31,  7.90it/s]\n",
      " running test-auc = 0.51634\n",
      "running train-auc = 0.99953\n",
      "\n",
      " 80%|████████  | 4000/5000 [08:44<02:12,  7.53it/s]\n",
      " running test-auc = 0.54684\n",
      "running train-auc = 0.99860\n",
      "\n",
      " 84%|████████▍ | 4200/5000 [09:10<01:42,  7.77it/s]\n",
      " running test-auc = 0.56754\n",
      "running train-auc = 0.99860\n",
      "\n",
      " 88%|████████▊ | 4400/5000 [09:36<01:17,  7.74it/s]\n",
      " running test-auc = 0.61983\n",
      "running train-auc = 0.99953\n",
      "\n",
      " 92%|█████████▏| 4600/5000 [10:02<00:50,  7.87it/s]\n",
      " running test-auc = 0.56209\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 96%|█████████▌| 4800/5000 [10:28<00:26,  7.68it/s]\n",
      " running test-auc = 0.56863\n",
      "running train-auc = 0.98691\n",
      "\n",
      "100%|██████████| 5000/5000 [10:54<00:00,  7.64it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.58170, acr = 0.59091\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.545455, auc is 0.571429,  fpr is 0.642857\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.545455, auc is 0.564286,  fpr is 0.714286\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.568182, auc is 0.466667,  fpr is 0.714286\n",
      "for justmode: accuracy is 0.681818, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.545455, fpr is 0.428571\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.37619\n",
      "running train-auc = 1.00000\n",
      "\n",
      "  4%|▍         | 200/5000 [00:27<11:12,  7.14it/s]\n",
      " running test-auc = 0.69286\n",
      "running train-auc = 1.00000\n",
      "\n",
      "  8%|▊         | 400/5000 [00:54<10:41,  7.17it/s]\n",
      " running test-auc = 0.52857\n",
      "running train-auc = 0.98708\n",
      "\n",
      " 12%|█▏        | 600/5000 [01:20<09:14,  7.93it/s]\n",
      " running test-auc = 0.54881\n",
      "running train-auc = 0.99554\n",
      "\n",
      " 16%|█▌        | 800/5000 [01:46<09:06,  7.69it/s]\n",
      " running test-auc = 0.54048\n",
      "running train-auc = 0.99911\n",
      "\n",
      " 20%|██        | 1000/5000 [02:12<08:22,  7.96it/s]\n",
      " running test-auc = 0.49762\n",
      "running train-auc = 0.99866\n",
      "\n",
      " 24%|██▍       | 1200/5000 [02:37<08:07,  7.80it/s]\n",
      " running test-auc = 0.48810\n",
      "running train-auc = 0.99733\n",
      "\n",
      " 28%|██▊       | 1400/5000 [03:03<07:38,  7.84it/s]\n",
      " running test-auc = 0.65238\n",
      "running train-auc = 0.99421\n",
      "\n",
      " 32%|███▏      | 1600/5000 [03:28<07:20,  7.72it/s]\n",
      " running test-auc = 0.50714\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 36%|███▌      | 1800/5000 [03:54<06:46,  7.88it/s]\n",
      " running test-auc = 0.40476\n",
      "running train-auc = 0.99866\n",
      "\n",
      " 40%|████      | 2000/5000 [04:19<06:31,  7.67it/s]\n",
      " running test-auc = 0.40714\n",
      "running train-auc = 0.99554\n",
      "\n",
      " 44%|████▍     | 2200/5000 [04:46<06:20,  7.36it/s]\n",
      " running test-auc = 0.42619\n",
      "running train-auc = 0.99866\n",
      "\n",
      " 48%|████▊     | 2400/5000 [05:13<05:34,  7.78it/s]\n",
      " running test-auc = 0.41190\n",
      "running train-auc = 0.99777\n",
      "\n",
      " 52%|█████▏    | 2600/5000 [05:38<04:55,  8.13it/s]\n",
      " running test-auc = 0.67381\n",
      "running train-auc = 0.99866\n",
      "\n",
      " 56%|█████▌    | 2800/5000 [06:04<04:44,  7.74it/s]\n",
      " running test-auc = 0.59643\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 60%|██████    | 3000/5000 [06:30<04:11,  7.95it/s]\n",
      " running test-auc = 0.54167\n",
      "running train-auc = 0.99242\n",
      "\n",
      " 64%|██████▍   | 3200/5000 [06:56<03:50,  7.80it/s]\n",
      " running test-auc = 0.52262\n",
      "running train-auc = 0.99643\n",
      "\n",
      " 68%|██████▊   | 3400/5000 [07:22<03:37,  7.36it/s]\n",
      " running test-auc = 0.54762\n",
      "running train-auc = 0.99911\n",
      "\n",
      " 72%|███████▏  | 3600/5000 [07:48<02:56,  7.91it/s]\n",
      " running test-auc = 0.61548\n",
      "running train-auc = 0.98663\n",
      "\n",
      " 76%|███████▌  | 3800/5000 [08:15<02:37,  7.60it/s]\n",
      " running test-auc = 0.50238\n",
      "running train-auc = 0.99421\n",
      "\n",
      " 80%|████████  | 4000/5000 [08:42<02:09,  7.70it/s]\n",
      " running test-auc = 0.42738\n",
      "running train-auc = 0.99777\n",
      "\n",
      " 84%|████████▍ | 4200/5000 [09:09<01:44,  7.65it/s]\n",
      " running test-auc = 0.35476\n",
      "running train-auc = 0.99688\n",
      "\n",
      " 88%|████████▊ | 4400/5000 [09:35<01:18,  7.66it/s]\n",
      " running test-auc = 0.53571\n",
      "running train-auc = 0.99822\n",
      "\n",
      " 92%|█████████▏| 4600/5000 [10:02<00:53,  7.43it/s]\n",
      " running test-auc = 0.34048\n",
      "running train-auc = 0.99733\n",
      "\n",
      " 96%|█████████▌| 4800/5000 [10:29<00:26,  7.55it/s]\n",
      " running test-auc = 0.45595\n",
      "running train-auc = 0.99955\n",
      "\n",
      "100%|██████████| 5000/5000 [10:55<00:00,  7.62it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.56667, acr = 0.54545\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.681818, auc is 0.685950,  fpr is 0.454545\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.750000, auc is 0.680441,  fpr is 0.363636\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.522727, auc is 0.545455,  fpr is 0.545455\n",
      "for justmode: accuracy is 0.750000, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.477273, fpr is 0.454545\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.66942\n",
      "running train-auc = 0.99571\n",
      "\n",
      "  4%|▍         | 200/5000 [00:26<10:13,  7.83it/s]\n",
      " running test-auc = 0.54959\n",
      "running train-auc = 0.99743\n",
      "\n",
      "  8%|▊         | 400/5000 [00:53<09:59,  7.67it/s]\n",
      " running test-auc = 0.51515\n",
      "running train-auc = 0.99871\n",
      "\n",
      " 12%|█▏        | 600/5000 [01:20<10:06,  7.26it/s]\n",
      " running test-auc = 0.41598\n",
      "running train-auc = 0.99485\n",
      "\n",
      " 16%|█▌        | 800/5000 [01:47<09:39,  7.25it/s]\n",
      " running test-auc = 0.70661\n",
      "running train-auc = 0.99914\n",
      "\n",
      " 20%|██        | 1000/5000 [02:14<09:12,  7.24it/s]\n",
      " running test-auc = 0.69972\n",
      "running train-auc = 0.99314\n",
      "\n",
      " 24%|██▍       | 1200/5000 [02:41<08:23,  7.55it/s]\n",
      " running test-auc = 0.80579\n",
      "running train-auc = 0.98498\n",
      "\n",
      " 28%|██▊       | 1400/5000 [03:08<08:08,  7.37it/s]\n",
      " running test-auc = 0.61433\n",
      "running train-auc = 0.99785\n",
      "\n",
      " 32%|███▏      | 1600/5000 [03:36<07:57,  7.13it/s]\n",
      " running test-auc = 0.69421\n",
      "running train-auc = 0.99314\n",
      "\n",
      " 36%|███▌      | 1800/5000 [04:03<07:12,  7.40it/s]\n",
      " running test-auc = 0.71901\n",
      "running train-auc = 0.98584\n",
      "\n",
      " 40%|████      | 2000/5000 [04:31<06:54,  7.24it/s]\n",
      " running test-auc = 0.56061\n",
      "running train-auc = 0.99571\n",
      "\n",
      " 44%|████▍     | 2200/5000 [04:58<06:24,  7.28it/s]\n",
      " running test-auc = 0.57300\n",
      "running train-auc = 0.99657\n",
      "\n",
      " 48%|████▊     | 2400/5000 [05:26<05:47,  7.48it/s]\n",
      " running test-auc = 0.55647\n",
      "running train-auc = 0.99485\n",
      "\n",
      " 52%|█████▏    | 2600/5000 [05:53<05:11,  7.72it/s]\n",
      " running test-auc = 0.58678\n",
      "running train-auc = 0.99743\n",
      "\n",
      " 56%|█████▌    | 2800/5000 [06:20<05:12,  7.05it/s]\n",
      " running test-auc = 0.71763\n",
      "running train-auc = 0.99828\n",
      "\n",
      " 60%|██████    | 3000/5000 [06:49<04:29,  7.41it/s]\n",
      " running test-auc = 0.69421\n",
      "running train-auc = 0.99571\n",
      "\n",
      " 64%|██████▍   | 3200/5000 [07:16<04:09,  7.22it/s]\n",
      " running test-auc = 0.64738\n",
      "running train-auc = 0.99871\n",
      "\n",
      " 68%|██████▊   | 3400/5000 [07:44<03:47,  7.04it/s]\n",
      " running test-auc = 0.72176\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 72%|███████▏  | 3600/5000 [08:12<03:07,  7.47it/s]\n",
      " running test-auc = 0.57300\n",
      "running train-auc = 0.99700\n",
      "\n",
      " 76%|███████▌  | 3800/5000 [08:38<02:31,  7.91it/s]\n",
      " running test-auc = 0.62534\n",
      "running train-auc = 0.99743\n",
      "\n",
      " 80%|████████  | 4000/5000 [09:04<02:10,  7.66it/s]\n",
      " running test-auc = 0.69284\n",
      "running train-auc = 0.99957\n",
      "\n",
      " 84%|████████▍ | 4200/5000 [09:30<01:43,  7.70it/s]\n",
      " running test-auc = 0.56061\n",
      "running train-auc = 0.99442\n",
      "\n",
      " 88%|████████▊ | 4400/5000 [09:56<01:19,  7.58it/s]\n",
      " running test-auc = 0.57576\n",
      "running train-auc = 0.99785\n",
      "\n",
      " 92%|█████████▏| 4600/5000 [10:22<00:50,  7.91it/s]\n",
      " running test-auc = 0.58264\n",
      "running train-auc = 0.99957\n",
      "\n",
      " 96%|█████████▌| 4800/5000 [10:49<00:25,  7.71it/s]\n",
      " running test-auc = 0.60744\n",
      "running train-auc = 0.99914\n",
      "\n",
      "100%|██████████| 5000/5000 [11:14<00:00,  7.41it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.55234, acr = 0.47727\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.613636, auc is 0.602941,  fpr is 0.500000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.659091, auc is 0.632353,  fpr is 0.400000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.704545, auc is 0.610294,  fpr is 0.600000\n",
      "for justmode: accuracy is 0.772727, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.590909, fpr is 0.200000\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.41029\n",
      "running train-auc = 0.99873\n",
      "\n",
      "  4%|▍         | 200/5000 [00:26<11:03,  7.23it/s]\n",
      " running test-auc = 0.63676\n",
      "running train-auc = 0.99703\n",
      "\n",
      "  8%|▊         | 400/5000 [00:53<10:11,  7.52it/s]\n",
      " running test-auc = 0.51471\n",
      "running train-auc = 0.99958\n",
      "\n",
      " 12%|█▏        | 600/5000 [01:20<09:40,  7.58it/s]\n",
      " running test-auc = 0.65588\n",
      "running train-auc = 0.99873\n",
      "\n",
      " 16%|█▌        | 800/5000 [01:46<09:24,  7.44it/s]\n",
      " running test-auc = 0.53382\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 20%|██        | 1000/5000 [02:13<08:33,  7.80it/s]\n",
      " running test-auc = 0.55000\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 24%|██▍       | 1200/5000 [02:39<08:37,  7.34it/s]\n",
      " running test-auc = 0.78824\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 28%|██▊       | 1400/5000 [03:06<07:44,  7.74it/s]\n",
      " running test-auc = 0.65294\n",
      "running train-auc = 0.99363\n",
      "\n",
      " 32%|███▏      | 1600/5000 [03:34<07:58,  7.11it/s]\n",
      " running test-auc = 0.55294\n",
      "running train-auc = 0.99618\n",
      "\n",
      " 36%|███▌      | 1800/5000 [04:02<07:33,  7.06it/s]\n",
      " running test-auc = 0.58235\n",
      "running train-auc = 0.99363\n",
      "\n",
      " 40%|████      | 2000/5000 [04:30<06:52,  7.27it/s]\n",
      " running test-auc = 0.54412\n",
      "running train-auc = 0.99830\n",
      "\n",
      " 44%|████▍     | 2200/5000 [04:58<06:41,  6.98it/s]\n",
      " running test-auc = 0.55882\n",
      "running train-auc = 0.99830\n",
      "\n",
      " 48%|████▊     | 2400/5000 [05:27<06:41,  6.48it/s]\n",
      " running test-auc = 0.63382\n",
      "running train-auc = 0.99660\n",
      "\n",
      " 52%|█████▏    | 2600/5000 [05:55<05:30,  7.27it/s]\n",
      " running test-auc = 0.62059\n",
      "running train-auc = 0.99491\n",
      "\n",
      " 56%|█████▌    | 2800/5000 [06:23<05:17,  6.94it/s]\n",
      " running test-auc = 0.60441\n",
      "running train-auc = 0.99830\n",
      "\n",
      " 60%|██████    | 3000/5000 [06:52<05:01,  6.64it/s]\n",
      " running test-auc = 0.65588\n",
      "running train-auc = 0.99745\n",
      "\n",
      " 64%|██████▍   | 3200/5000 [07:20<04:02,  7.43it/s]\n",
      " running test-auc = 0.64412\n",
      "running train-auc = 0.99278\n",
      "\n",
      " 68%|██████▊   | 3400/5000 [07:47<03:47,  7.03it/s]\n",
      " running test-auc = 0.63088\n",
      "running train-auc = 0.99915\n",
      "\n",
      " 72%|███████▏  | 3600/5000 [08:15<03:08,  7.42it/s]\n",
      " running test-auc = 0.62647\n",
      "running train-auc = 0.99958\n",
      "\n",
      " 76%|███████▌  | 3800/5000 [08:43<02:54,  6.89it/s]\n",
      " running test-auc = 0.64853\n",
      "running train-auc = 0.99873\n",
      "\n",
      " 80%|████████  | 4000/5000 [09:11<02:11,  7.63it/s]\n",
      " running test-auc = 0.64118\n",
      "running train-auc = 0.99321\n",
      "\n",
      " 84%|████████▍ | 4200/5000 [09:38<01:48,  7.38it/s]\n",
      " running test-auc = 0.72647\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 88%|████████▊ | 4400/5000 [10:06<01:26,  6.97it/s]\n",
      " running test-auc = 0.54853\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 92%|█████████▏| 4600/5000 [10:33<00:54,  7.30it/s]\n",
      " running test-auc = 0.57941\n",
      "running train-auc = 0.99576\n",
      "\n",
      " 96%|█████████▌| 4800/5000 [11:01<00:27,  7.16it/s]\n",
      " running test-auc = 0.55000\n",
      "running train-auc = 0.99745\n",
      "\n",
      "100%|██████████| 5000/5000 [11:29<00:00,  7.25it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.56176, acr = 0.56818\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.659091, auc is 0.721133,  fpr is 0.352941\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "for LogitElsnet: accuracy is 0.636364, auc is 0.716776,  fpr is 0.411765\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.636364, auc is 0.504357,  fpr is 0.588235\n",
      "for justmode: accuracy is 0.613636, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.613636, fpr is 0.529412\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.53377\n",
      "running train-auc = 0.99813\n",
      "\n",
      "  4%|▍         | 200/5000 [00:27<10:10,  7.87it/s]\n",
      " running test-auc = 0.71569\n",
      "running train-auc = 0.99486\n",
      "\n",
      "  8%|▊         | 400/5000 [00:53<09:58,  7.68it/s]\n",
      " running test-auc = 0.56972\n",
      "running train-auc = 0.99953\n",
      "\n",
      " 12%|█▏        | 600/5000 [01:20<09:50,  7.46it/s]\n",
      " running test-auc = 0.58824\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 16%|█▌        | 800/5000 [01:47<09:36,  7.28it/s]\n",
      " running test-auc = 0.67647\n",
      "running train-auc = 0.98551\n",
      "\n",
      " 20%|██        | 1000/5000 [02:14<08:58,  7.43it/s]\n",
      " running test-auc = 0.52941\n",
      "running train-auc = 0.99860\n",
      "\n",
      " 24%|██▍       | 1200/5000 [02:44<08:52,  7.14it/s]\n",
      " running test-auc = 0.71678\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 28%|██▊       | 1400/5000 [03:13<08:25,  7.13it/s]\n",
      " running test-auc = 0.63617\n",
      "running train-auc = 0.99673\n",
      "\n",
      " 32%|███▏      | 1600/5000 [03:42<08:18,  6.82it/s]\n",
      " running test-auc = 0.55773\n",
      "running train-auc = 0.99953\n",
      "\n",
      " 36%|███▌      | 1800/5000 [04:11<07:44,  6.89it/s]\n",
      " running test-auc = 0.62092\n",
      "running train-auc = 0.99719\n",
      "\n",
      " 40%|████      | 2000/5000 [04:40<07:01,  7.12it/s]\n",
      " running test-auc = 0.66449\n",
      "running train-auc = 1.00000\n",
      "\n",
      " 44%|████▍     | 2200/5000 [05:08<06:27,  7.23it/s]\n",
      " running test-auc = 0.61002\n",
      "running train-auc = 0.99532\n",
      "\n",
      " 48%|████▊     | 2400/5000 [05:37<06:03,  7.16it/s]\n",
      " running test-auc = 0.60784\n",
      "running train-auc = 0.99906\n",
      "\n",
      " 52%|█████▏    | 2600/5000 [06:05<05:43,  6.98it/s]\n",
      " running test-auc = 0.45969\n",
      "running train-auc = 0.99579\n",
      "\n",
      " 56%|█████▌    | 2800/5000 [06:33<05:08,  7.14it/s]\n",
      " running test-auc = 0.66558\n",
      "running train-auc = 0.98738\n",
      "\n",
      " 60%|██████    | 3000/5000 [07:02<04:43,  7.05it/s]\n",
      " running test-auc = 0.66885\n",
      "running train-auc = 0.99018\n",
      "\n",
      " 64%|██████▍   | 3200/5000 [07:31<04:25,  6.79it/s]\n",
      " running test-auc = 0.62092\n",
      "running train-auc = 0.99953\n",
      "\n",
      " 68%|██████▊   | 3400/5000 [08:00<03:51,  6.90it/s]\n",
      " running test-auc = 0.66667\n",
      "running train-auc = 0.99439\n",
      "\n",
      " 72%|███████▏  | 3600/5000 [08:28<03:20,  6.98it/s]\n",
      " running test-auc = 0.66449\n",
      "running train-auc = 0.99532\n",
      "\n",
      " 76%|███████▌  | 3800/5000 [08:57<02:56,  6.81it/s]\n",
      " running test-auc = 0.61220\n",
      "running train-auc = 0.99345\n",
      "\n",
      " 80%|████████  | 4000/5000 [09:26<02:24,  6.92it/s]\n",
      " running test-auc = 0.59477\n",
      "running train-auc = 0.99906\n",
      "\n",
      " 84%|████████▍ | 4200/5000 [09:55<01:55,  6.94it/s]\n",
      " running test-auc = 0.78867\n",
      "running train-auc = 0.99766\n",
      "\n",
      " 88%|████████▊ | 4400/5000 [10:23<01:27,  6.85it/s]\n",
      " running test-auc = 0.47712\n",
      "running train-auc = 0.98925\n",
      "\n",
      " 92%|█████████▏| 4600/5000 [10:52<00:57,  6.91it/s]\n",
      " running test-auc = 0.59259\n",
      "running train-auc = 0.99673\n",
      "\n",
      " 96%|█████████▌| 4800/5000 [11:20<00:28,  7.05it/s]\n",
      " running test-auc = 0.62963\n",
      "running train-auc = 0.99766\n",
      "\n",
      "100%|██████████| 5000/5000 [11:49<00:00,  7.05it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.65686, acr = 0.63636\n",
      "\n",
      "\n",
      "ours_acr_mean: 0.5318,ours_acr_std: 0.0530 \n",
      "lr_acr_mean: 0.6045,lr_acr_std: 0.0540 \n",
      "esnet_acr_mean: 0.6114,esnet_acr_std: 0.0630 \n",
      "rf_acr_mean: 0.6114,rf_acr_mean: 0.0523 \n",
      "just-mode_acr_mean: 0.6591,mode_acr_std: 0.0826 \n",
      "just-random_acr_mean: 0.5091,just-random_acr_std: 0.0846 \n",
      "ours_AUC_mean: 0.5391,ours_AUC_std: 0.0621 \n",
      "lr_AUC_mean: 0.6075,lr_AUC_std: 0.0691 \n",
      "esnet_AUC_mean: 0.6162,esnet_AUC_mean: 0.0715 \n",
      "rf_AUC_mean: 0.6023,rf_AUC_std: 0.0692 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = 10\n",
    "lr_acc = np.zeros(N)\n",
    "rf_acc = np.zeros(N)\n",
    "esnet_acc = np.zeros(N)\n",
    "mode_acc = np.zeros(N)\n",
    "random_acc = np.zeros(N)\n",
    "ours_acc = np.zeros(N)\n",
    "\n",
    "\n",
    "lr_auc = np.zeros(N)\n",
    "rf_auc = np.zeros(N)\n",
    "esnet_auc = np.zeros(N)\n",
    "ours_auc = np.zeros(N)\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, Y.squeeze(),test_size=0.3)\n",
    "\n",
    "    data_dict = {'X_tr':X_train, 'y_tr':y_train, 'X_test':X_test, 'y_test':y_test}  \n",
    "    dict_lr = baseline_LogitLR(data_dict)\n",
    "    dict_els = baseline_LogitElsnet(data_dict)\n",
    "    dict_rf = baseline_RanForest(data_dict)\n",
    "    dict_mode = baseline_justmode(data_dict)\n",
    "    dict_random = baseline_random(data_dict)\n",
    "\n",
    "    model = Gibbs_sampling(data_dict,get_init_paras(dict_lr['clf'].coef_.squeeze()), hyper_paras)\n",
    "    dict_ours = model.model_run()\n",
    "\n",
    "    lr_acc[i] = dict_lr['acr']\n",
    "    esnet_acc[i] = dict_els['acr']\n",
    "    rf_acc[i] = dict_rf['acr']\n",
    "    mode_acc[i] = dict_mode['acr']\n",
    "    random_acc[i] = dict_random['acr']\n",
    "    ours_acc[i] = dict_ours['acr']\n",
    "\n",
    "\n",
    "    lr_auc[i] = dict_lr['auc']\n",
    "    rf_auc[i] = dict_rf['auc']\n",
    "    esnet_auc[i] = dict_els['auc']\n",
    "    ours_auc[i] = dict_ours['auc']\n",
    "\n",
    "\n",
    "print('\\n\\nours_acr_mean: %.4f,ours_acr_std: %.4f '%( ours_acc.mean(), ours_acc.std() ) )\n",
    "print('lr_acr_mean: %.4f,lr_acr_std: %.4f '%(lr_acc.mean(),lr_acc.std() ) )\n",
    "print('esnet_acr_mean: %.4f,esnet_acr_std: %.4f '%(esnet_acc.mean(),esnet_acc.std() ) )\n",
    "print('rf_acr_mean: %.4f,rf_acr_mean: %.4f '%(rf_acc.mean(),rf_acc.std() ) )\n",
    "print('just-mode_acr_mean: %.4f,mode_acr_std: %.4f '%(mode_acc.mean(),mode_acc.std() ) )\n",
    "print('just-random_acr_mean: %.4f,just-random_acr_std: %.4f '%(random_acc.mean(),random_acc.std() ) )\n",
    "\n",
    "print('ours_AUC_mean: %.4f,ours_AUC_std: %.4f '%(ours_auc.mean(),ours_auc.std() ) )\n",
    "print('lr_AUC_mean: %.4f,lr_AUC_std: %.4f '%(lr_auc.mean(),lr_auc.std() ) )\n",
    "print('esnet_AUC_mean: %.4f,esnet_AUC_mean: %.4f '%(esnet_auc.mean(),esnet_auc.std() ) )\n",
    "print('rf_AUC_mean: %.4f,rf_AUC_std: %.4f '%(rf_auc.mean(),rf_auc.std() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lr_rmse_mean: 1.3432,lr_rmse_std: 0.1101 \nesnet_rmse_mean: 1.1714,esnet_rmse_std: 0.2027 \nours_rmse_mean: 1.1854,ours_rmse_std: 0.1600 \n"
     ]
    }
   ],
   "source": [
    "print('lr_rmse_mean: %.4f,lr_rmse_std: %.4f '%(lr_rmse[[0,2,3]].mean(),lr_rmse[[0,2,3]].std() ) )\n",
    "print('esnet_rmse_mean: %.4f,esnet_rmse_std: %.4f '%(esnet_rmse[[0,2,3]].mean(),esnet_rmse[[0,2,3]].std() ) )\n",
    "print('ours_rmse_mean: %.4f,ours_rmse_std: %.4f '%(ours_rmse[[0,2,3]].mean(),ours_rmse[[0,2,3]].std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([ 0, 24, 26], dtype=int64),)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model.z_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "with elanet, rmse is 0.92433\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "clf = ElasticNet(alpha = 0.2,l1_ratio =0.1, fit_intercept=False)\n",
    "clf.fit(data_dict['X_tr'], data_dict['y_tr'])\n",
    "predict = clf.predict(data_dict['X_test']).squeeze()\n",
    "rmse = np.sqrt(np.mean((predict-data_dict['y_test'].squeeze())**2))\n",
    "print('with elanet, rmse is %.5f'%(rmse))\n",
    "len(clf.coef_[clf.coef_>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.02290048, -0.00710251, -0.        ,\n",
       "       -0.        , -0.00906848, -0.        , -0.00073689, -0.        ,\n",
       "       -0.01301175,  0.        ,  0.        , -0.        ,  0.04461527,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.04582258,\n",
       "       -0.        , -0.        ,  0.        , -0.00123495, -0.00794974,\n",
       "       -0.01668715, -0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.05419244,  0.        , -0.        , -0.05123613,  0.00319208,\n",
       "       -0.11459053,  0.05525388,  0.        ,  0.02155437,  0.        ,\n",
       "        0.02790468,  0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.00617801,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.03845294,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.04606137, -0.        , -0.04967678,\n",
       "       -0.0510887 , -0.03915234,  0.        , -0.024777  ,  0.03848892,\n",
       "       -0.02260924,  0.0034599 , -0.01779437, -0.15154597,  0.08939166,\n",
       "        0.        ,  0.07388046,  0.00081879,  0.01024509,  0.        ,\n",
       "        0.00782891,  0.        ,  0.03999645,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.0009416 , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.03962395, -0.01585616, -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.00148079,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.03311627,  0.02364622,  0.02364671,  0.        ,\n",
       "        0.02881839, -0.        ,  0.01989795,  0.        ,  0.        ,\n",
       "        0.        ,  0.02707263, -0.03641279, -0.        ,  0.00471888,\n",
       "        0.01585947,  0.        ,  0.        ,  0.01715877,  0.        ,\n",
       "        0.00780515,  0.02258382, -0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.05046656,\n",
       "        0.00712515, -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.00911781,  0.        ,  0.0308587 ,  0.        ,  0.12240987,\n",
       "        0.        , -0.        ,  0.01683971, -0.        , -0.0175792 ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.02226112,\n",
       "        0.        , -0.        , -0.01948966, -0.        , -0.00655018,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.01214122,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.00299604,  0.        ,\n",
       "        0.        ,  0.01184025, -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.12643198, -0.        ,  0.03102914,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        , -0.02904414,\n",
       "       -0.        , -0.        ,  0.00792165, -0.        , -0.        ,\n",
       "        0.        , -0.01458936, -0.        , -0.00493862, -0.        ,\n",
       "       -0.        , -0.03742177, -0.01490766, -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.02930586,\n",
       "       -0.        , -0.0035898 ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.04591052, -0.00539842, -0.        , -0.00423613,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.0044806 , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.01892325, -0.        , -0.        ,\n",
       "       -0.17849845,  0.01033251,  0.        , -0.        ,  0.02962639,\n",
       "        0.        , -0.        , -0.00563414, -0.        ,  0.        ,\n",
       "        0.00985186,  0.        ,  0.00467639,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.04907754, -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.00513774, -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.04629642,  0.02493082,  0.02733485,  0.        ,\n",
       "        0.05399662,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.03792165,\n",
       "        0.10768513,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.01404093,  0.        ,  0.        ,  0.        ,  0.02202576,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.01181202, -0.        , -0.        , -0.01236265, -0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.00821121,\n",
       "       -0.02238252, -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.00168103,  0.        , -0.        , -0.10912436,  0.        ,\n",
       "       -0.        ,  0.        , -0.06424858, -0.        , -0.        ,\n",
       "        0.10415519,  0.04376389,  0.01697914,  0.        ,  0.        ,\n",
       "       -0.01312378,  0.03672591,  0.        , -0.03417803,  0.        ,\n",
       "        0.        ,  0.02856535, -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.02499551,  0.        , -0.        ,\n",
       "       -0.02190513,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.05397044, -0.        , -0.        , -0.04215188,\n",
       "       -0.04784062, -0.        , -0.01406243, -0.        , -0.02008678,\n",
       "       -0.03620158, -0.        , -0.01014058, -0.08876233,  0.11709101,\n",
       "       -0.03473525, -0.02286894,  0.        ,  0.05516396, -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.05392373,  0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.02025788,  0.        ,  0.        ,\n",
       "        0.02495222,  0.02837319,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.03277729,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.01735679, -0.01053047, -0.        ,\n",
       "       -0.        ,  0.        ,  0.10418004,  0.02373764,  0.        ,\n",
       "       -0.        , -0.0439794 , -0.        ,  0.03669145,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.00784269,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.02417521,  0.        ,  0.01993967,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.02476074,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.0255349 ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.02190399,\n",
       "       -0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.08300116, -0.01377398,  0.06784959, -0.0311479 ,\n",
       "       -0.06275233, -0.03652226, -0.        ,  0.        ,  0.        ,\n",
       "        0.06701617,  0.02434526, -0.        , -0.        , -0.        ,\n",
       "       -0.01651747, -0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.04118781,\n",
       "       -0.00059209, -0.        ,  0.        ,  0.00142739,  0.01374944,\n",
       "       -0.02384531, -0.15420052, -0.06653698, -0.        ,  0.        ,\n",
       "        0.1033298 ,  0.06381967,  0.        ,  0.03097225, -0.        ,\n",
       "       -0.01842837, -0.03546417, -0.00226203, -0.        ,  0.        ,\n",
       "       -0.00353995, -0.00972607, -0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.05025683,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.00739962,  0.        ,  0.        ,  0.02112023,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.0034799 ,\n",
       "       -0.00728382,  0.        , -0.03972991, -0.        , -0.00405776,\n",
       "       -0.        , -0.0639066 , -0.11904907, -0.        ,  0.00150147,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.03250672,\n",
       "        0.00436798,  0.        ,  0.        ,  0.0348344 ,  0.08700881,\n",
       "        0.        ,  0.00404911,  0.        ,  0.        ,  0.00285669,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.02295848,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.01329357, -0.        ,  0.0257707 ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.00984572,  0.        ,  0.03657566,\n",
       "        0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.08141421,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "w_es"
   ]
  }
 ]
}