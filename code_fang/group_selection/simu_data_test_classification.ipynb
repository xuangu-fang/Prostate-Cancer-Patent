{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python374jvsc74a57bd06ee6abd9b0d44c31108424068010123bef58fe758ec0be567da6fa0551538e82",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "cb03ba31accc2f25e369d1aa3152af3569125a23627c23321dfacf015e0ff229"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy\n",
    "# import xlrd \n",
    "import sklearn\n",
    "\n",
    "from Gibbs_model_probit import Gibbs_sampling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import multivariate_normal\n",
    "from utils import baseline_lr,baseline_esnet,baseline_justmean\n",
    "from utils import baseline_LogitElsnet,baseline_justmode,baseline_random,baseline_LogitLR,baseline_RanForest\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import binom \n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1000, 50) (1000,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# generate simu data\n",
    "N_sample = 1000\n",
    "N_feature_t = 6\n",
    "N_feature_f = 44\n",
    "noise_level = 0\n",
    "N_feature = N_feature_t + N_feature_f\n",
    "\n",
    "feature_t = np.random.normal(0, 1, size=(N_sample, N_feature_t)) # true feature (should be select)\n",
    "feature_f = np.random.normal(1, 0.5, size=(N_sample, N_feature_f)) # false feaures\n",
    "\n",
    "W_t = np.array([-3, 3, -2, 2, -1, 1])#np.random.normal(0, 1, size=(N_feature_t,1))\n",
    "\n",
    "Y = np.matmul(feature_t, W_t)\n",
    "\n",
    "Y = Y + noise_level*np.random.normal(0, 1, size=Y.shape)\n",
    "Y = np.where(Y>0,1,0)\n",
    "\n",
    "X = np.concatenate((feature_t,feature_f),1)\n",
    "print(X.shape,Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0, 1, 6, 7, 8],\n",
       " [2, 3, 9, 10, 11],\n",
       " [4, 5, 12, 13, 14],\n",
       " [15, 16, 17, 18, 19],\n",
       " [20, 21, 22, 23, 24],\n",
       " [25, 26, 27, 28, 29],\n",
       " [30, 31, 32, 33, 34],\n",
       " [35, 36, 37, 38, 39],\n",
       " [40, 41, 42, 43, 44],\n",
       " [45, 46, 47, 48, 49]]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# random split the group\n",
    "K = 10 # nunber of group\n",
    "group_ind = []\n",
    "\n",
    "'''\n",
    "for i in range(K):\n",
    "    N_t = N_feature_t//K\n",
    "    N_f = N_feature_f//K\n",
    "    idx_t = [N_t*i +j for j in range(N_t)]\n",
    "    idx_f = [N_feature_t + N_f*i +j for j in range(N_f)]\n",
    "    group_ind.append(idx_t + idx_f)\n",
    "\n",
    "# so in each group, first 10 idx are the ture features\n",
    "'''\n",
    "for i in range(K):\n",
    "    if i < 3:\n",
    "        N_t = 2 #N_feature_t//K\n",
    "        N_f = 3 #N_feature_f//K\n",
    "        idx_t = [N_t*i +j for j in range(N_t)]\n",
    "        idx_f = [N_feature_t + N_f*i +j for j in range(N_f)]\n",
    "        group_ind.append(idx_t + idx_f)\n",
    "    else:\n",
    "        N_f = 5\n",
    "        idx_f = [ N_f*i +j for j in range(N_f)]\n",
    "        group_ind.append(idx_f)\n",
    "\n",
    "group_ind\n",
    "# 0-5 : relevant features\n",
    "# 6-40: unrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1000, 51)\n"
     ]
    }
   ],
   "source": [
    "# re-arrange the features of X based on the group split order\n",
    "X_new = np.concatenate([X[:,group_ind[i]] for i in range(K)],axis=1)\n",
    "\n",
    "# add all-one column at the last \n",
    "bias_col = np.ones(N_sample).reshape((N_sample,1))\n",
    "X_new = np.concatenate((X_new,bias_col),axis=1)\n",
    "\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, Y.squeeze(),test_size=0.3)\n",
    "data_dict = {'X_tr':X_train, 'y_tr':y_train, 'X_test':X_test, 'y_test':y_test}                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init hyper-parameters\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "r0 = 1e-3\n",
    "r1 = 1.0\n",
    "a0 = 1.0\n",
    "b0 = 1.0\n",
    "JITTER = 1e-3\n",
    "\n",
    "INTERVAL = 100\n",
    "VALITA_INTERVAL = 500\n",
    "BURNING = 2000\n",
    "MAX_NUMBER = 3000\n",
    "\n",
    "hyper_paras = {'INTERVAL':INTERVAL, 'BURNING':BURNING,'MAX_NUMBER':MAX_NUMBER,'VALITA_INTERVAL':VALITA_INTERVAL,\n",
    "'alpha':alpha, 'beta':beta,'r0':r0,'r1':r1,'JITTER':JITTER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init parameters with lr_result\n",
    "def get_init_paras(w_lr):\n",
    "    z_array_init = np.zeros(K) #np.random.binomial(size=K, n=1, p= alpha)\n",
    "    s_list_init = [np.zeros(len(item)) for item in group_ind]#[np.random.binomial(size=len(item), n=1, p= beta) for item in group_ind]\n",
    "    b_init = w_lr[-1]#np.random.normal(loc=0.0, scale=r1,size=None)\n",
    "    # tau_init = 1.0#np.random.gamma(shape=alpha, scale=1.0/beta, size=None)\n",
    "\n",
    "    W_init = []\n",
    "    offset=0\n",
    "    for i in range(K):\n",
    "        # mask1 = 1-z_array_init[i] * s_list_init[i]\n",
    "        # mask2 = z_array_init[i] * s_list_init[i]\n",
    "        # spike = np.random.normal(loc=0.0, scale=r0,size=len(s_list_init[i]))\n",
    "        # slab = np.random.normal(loc=0.0, scale=r1,size=len(s_list_init[i]))\n",
    "        # W_group = spike * mask1 + slab * mask2\n",
    "\n",
    "        \n",
    "        group_len = len(s_list_init[i])\n",
    "        W_group= w_lr[offset:offset+group_len]\n",
    "        offset = offset + group_len\n",
    "        W_init.append(W_group)\n",
    "\n",
    "    init_paras = {'z':z_array_init, 's':s_list_init, 'b':b_init,  'W':W_init,'a0':a0,'b0':b0}\n",
    "    return init_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.986667, auc is 0.998755,  fpr is 0.013514\n",
      "for LogitElsnet: accuracy is 0.986667, auc is 0.998800,  fpr is 0.013514\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.750000, auc is 0.835237,  fpr is 0.162162\n",
      "for justmode: accuracy is 0.506667, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.503333, fpr is 0.500000\n",
      "  0%|          | 0/3000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.98969\n",
      "running train-auc = 0.99244\n",
      "\n",
      " 16%|█▌        | 481/3000 [00:02<00:12, 196.48it/s]\n",
      " running test-auc = 0.99973\n",
      "running train-auc = 0.99912\n",
      "\n",
      " 33%|███▎      | 994/3000 [00:05<00:09, 205.93it/s]\n",
      " running test-auc = 0.99924\n",
      "running train-auc = 0.99930\n",
      "\n",
      " 50%|████▉     | 1492/3000 [00:07<00:07, 199.77it/s]\n",
      " running test-auc = 0.99929\n",
      "running train-auc = 0.99944\n",
      "\n",
      " 67%|██████▋   | 2000/3000 [00:10<00:05, 198.16it/s]\n",
      " running test-auc = 0.99920\n",
      "running train-auc = 0.99878\n",
      "\n",
      " 83%|████████▎ | 2496/3000 [00:12<00:02, 194.29it/s]\n",
      " running test-auc = 0.99987\n",
      "running train-auc = 0.99946\n",
      "\n",
      "100%|██████████| 3000/3000 [00:15<00:00, 194.06it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.99973, acr = 0.99333\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.956667, auc is 0.992661,  fpr is 0.054795\n",
      "for LogitElsnet: accuracy is 0.956667, auc is 0.993017,  fpr is 0.054795\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.793333, auc is 0.862302,  fpr is 0.171233\n",
      "for justmode: accuracy is 0.513333, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.533333, fpr is 0.390411\n",
      "  0%|          | 0/3000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.98946\n",
      "running train-auc = 0.99403\n",
      "\n",
      " 16%|█▌        | 482/3000 [00:02<00:12, 196.37it/s]\n",
      " running test-auc = 0.99915\n",
      "running train-auc = 0.99978\n",
      "\n",
      " 33%|███▎      | 989/3000 [00:05<00:10, 194.51it/s]\n",
      " running test-auc = 0.99818\n",
      "running train-auc = 0.99955\n",
      "\n",
      " 50%|████▉     | 1491/3000 [00:07<00:08, 184.87it/s]\n",
      " running test-auc = 0.99862\n",
      "running train-auc = 0.99897\n",
      "\n",
      " 66%|██████▋   | 1993/3000 [00:10<00:05, 187.96it/s]\n",
      " running test-auc = 0.99782\n",
      "running train-auc = 0.99952\n",
      "\n",
      " 83%|████████▎ | 2493/3000 [00:12<00:02, 196.17it/s]\n",
      " running test-auc = 0.99795\n",
      "running train-auc = 0.99930\n",
      "\n",
      "100%|██████████| 3000/3000 [00:15<00:00, 193.55it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.99715, acr = 0.96667\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.976667, auc is 0.996932,  fpr is 0.039216\n",
      "for LogitElsnet: accuracy is 0.976667, auc is 0.997065,  fpr is 0.039216\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.800000, auc is 0.872082,  fpr is 0.163399\n",
      "for justmode: accuracy is 0.490000, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.503333, fpr is 0.522876\n",
      "  0%|          | 0/3000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.98897\n",
      "running train-auc = 0.99323\n",
      "\n",
      " 16%|█▋        | 495/3000 [00:02<00:13, 191.98it/s]\n",
      " running test-auc = 0.99938\n",
      "running train-auc = 0.99986\n",
      "\n",
      " 33%|███▎      | 999/3000 [00:05<00:09, 200.15it/s]\n",
      " running test-auc = 0.99462\n",
      "running train-auc = 0.99830\n",
      "\n",
      " 50%|█████     | 1500/3000 [00:07<00:07, 200.37it/s]\n",
      " running test-auc = 0.99724\n",
      "running train-auc = 0.99934\n",
      "\n",
      " 66%|██████▌   | 1985/3000 [00:09<00:05, 201.82it/s]\n",
      " running test-auc = 0.99933\n",
      "running train-auc = 0.99925\n",
      "\n",
      " 83%|████████▎ | 2494/3000 [00:12<00:02, 207.44it/s]\n",
      " running test-auc = 0.99876\n",
      "running train-auc = 0.99953\n",
      "\n",
      "100%|██████████| 3000/3000 [00:14<00:00, 200.87it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.99853, acr = 0.96667\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.983333, auc is 0.998655,  fpr is 0.022059\n",
      "for LogitElsnet: accuracy is 0.986667, auc is 0.998610,  fpr is 0.022059\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.780000, auc is 0.889213,  fpr is 0.154412\n",
      "for justmode: accuracy is 0.546667, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.533333, fpr is 0.441176\n",
      "  0%|          | 0/3000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.98077\n",
      "running train-auc = 0.98380\n",
      "\n",
      " 16%|█▌        | 483/3000 [00:02<00:12, 198.34it/s]\n",
      " running test-auc = 0.99946\n",
      "running train-auc = 0.99933\n",
      "\n",
      " 33%|███▎      | 988/3000 [00:05<00:10, 188.62it/s]\n",
      " running test-auc = 0.99852\n",
      "running train-auc = 0.99929\n",
      "\n",
      " 50%|████▉     | 1496/3000 [00:07<00:07, 196.16it/s]\n",
      " running test-auc = 1.00000\n",
      "running train-auc = 0.99922\n",
      "\n",
      " 67%|██████▋   | 1999/3000 [00:10<00:05, 191.45it/s]\n",
      " running test-auc = 0.99861\n",
      "running train-auc = 0.99913\n",
      "\n",
      " 83%|████████▎ | 2486/3000 [00:13<00:02, 183.70it/s]\n",
      " running test-auc = 0.99861\n",
      "running train-auc = 0.99952\n",
      "\n",
      "100%|██████████| 3000/3000 [00:15<00:00, 190.09it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.99942, acr = 0.97000\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "for LogitLR: accuracy is 0.963333, auc is 0.993911,  fpr is 0.039735\n",
      "for LogitElsnet: accuracy is 0.963333, auc is 0.994489,  fpr is 0.039735\n",
      "C:\\Users\\Tianfan Wu\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "for RandomForest: accuracy is 0.786667, auc is 0.872150,  fpr is 0.192053\n",
      "for justmode: accuracy is 0.496667, fpr is 1.000000\n",
      "for random-guess: accuracy is 0.520000, fpr is 0.516556\n",
      "  0%|          | 0/3000 [00:00<?, ?it/s]\n",
      " running test-auc = 0.96858\n",
      "running train-auc = 0.98396\n",
      "\n",
      " 16%|█▋        | 493/3000 [00:02<00:14, 176.90it/s]\n",
      " running test-auc = 0.99707\n",
      "running train-auc = 0.99865\n",
      "\n",
      " 33%|███▎      | 998/3000 [00:05<00:10, 183.44it/s]\n",
      " running test-auc = 0.99831\n",
      "running train-auc = 0.99914\n",
      "\n",
      " 50%|█████     | 1500/3000 [00:08<00:08, 179.85it/s]\n",
      " running test-auc = 0.99849\n",
      "running train-auc = 0.99956\n",
      "\n",
      " 67%|██████▋   | 1996/3000 [00:10<00:05, 193.45it/s]\n",
      " running test-auc = 0.99640\n",
      "running train-auc = 0.99885\n",
      "\n",
      " 83%|████████▎ | 2500/3000 [00:13<00:02, 193.30it/s]\n",
      " running test-auc = 0.99867\n",
      "running train-auc = 0.99931\n",
      "\n",
      "100%|██████████| 3000/3000 [00:16<00:00, 184.70it/s]\n",
      "\n",
      "\n",
      " final test auc = 0.99716, acr = 0.96333\n",
      "\n",
      "\n",
      "ours_acr_mean: 0.9720,ours_acr_std: 0.0109 \n",
      "lr_acr_mean: 0.9733,lr_acr_std: 0.0115 \n",
      "esnet_acr_mean: 0.9740,esnet_acr_std: 0.0122 \n",
      "rf_acr_mean: 0.7820,rf_acr_mean: 0.0173 \n",
      "just-mode_acr_mean: 0.5107,mode_acr_std: 0.0197 \n",
      "just-random_acr_mean: 0.5187,just-random_acr_std: 0.0134 \n",
      "ours_AUC_mean: 0.9984,ours_AUC_std: 0.0011 \n",
      "lr_AUC_mean: 0.9962,lr_AUC_std: 0.0025 \n",
      "esnet_AUC_mean: 0.9964,esnet_AUC_mean: 0.0023 \n",
      "rf_AUC_mean: 0.8662,rf_AUC_std: 0.0177 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = 5\n",
    "lr_acc = np.zeros(N)\n",
    "rf_acc = np.zeros(N)\n",
    "esnet_acc = np.zeros(N)\n",
    "mode_acc = np.zeros(N)\n",
    "random_acc = np.zeros(N)\n",
    "ours_acc = np.zeros(N)\n",
    "\n",
    "\n",
    "lr_auc = np.zeros(N)\n",
    "rf_auc = np.zeros(N)\n",
    "esnet_auc = np.zeros(N)\n",
    "ours_auc = np.zeros(N)\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, Y.squeeze(),test_size=0.3)\n",
    "\n",
    "    data_dict = {'X_tr':X_train, 'y_tr':y_train, 'X_test':X_test, 'y_test':y_test}  \n",
    "    dict_lr = baseline_LogitLR(data_dict)\n",
    "    dict_els = baseline_LogitElsnet(data_dict)\n",
    "    dict_rf = baseline_RanForest(data_dict)\n",
    "    dict_mode = baseline_justmode(data_dict)\n",
    "    dict_random = baseline_random(data_dict)\n",
    "\n",
    "    model = Gibbs_sampling(data_dict,get_init_paras(dict_lr['clf'].coef_.squeeze()), hyper_paras)\n",
    "    dict_ours = model.model_run()\n",
    "\n",
    "    lr_acc[i] = dict_lr['acr']\n",
    "    esnet_acc[i] = dict_els['acr']\n",
    "    rf_acc[i] = dict_rf['acr']\n",
    "    mode_acc[i] = dict_mode['acr']\n",
    "    random_acc[i] = dict_random['acr']\n",
    "    ours_acc[i] = dict_ours['acr']\n",
    "\n",
    "\n",
    "    lr_auc[i] = dict_lr['auc']\n",
    "    rf_auc[i] = dict_rf['auc']\n",
    "    esnet_auc[i] = dict_els['auc']\n",
    "    ours_auc[i] = dict_ours['auc']\n",
    "\n",
    "\n",
    "print('\\n\\nours_acr_mean: %.4f,ours_acr_std: %.4f '%( ours_acc.mean(), ours_acc.std() ) )\n",
    "print('lr_acr_mean: %.4f,lr_acr_std: %.4f '%(lr_acc.mean(),lr_acc.std() ) )\n",
    "print('esnet_acr_mean: %.4f,esnet_acr_std: %.4f '%(esnet_acc.mean(),esnet_acc.std() ) )\n",
    "print('rf_acr_mean: %.4f,rf_acr_mean: %.4f '%(rf_acc.mean(),rf_acc.std() ) )\n",
    "print('just-mode_acr_mean: %.4f,mode_acr_std: %.4f '%(mode_acc.mean(),mode_acc.std() ) )\n",
    "print('just-random_acr_mean: %.4f,just-random_acr_std: %.4f '%(random_acc.mean(),random_acc.std() ) )\n",
    "\n",
    "print('ours_AUC_mean: %.4f,ours_AUC_std: %.4f '%(ours_auc.mean(),ours_auc.std() ) )\n",
    "print('lr_AUC_mean: %.4f,lr_AUC_std: %.4f '%(lr_auc.mean(),lr_auc.std() ) )\n",
    "print('esnet_AUC_mean: %.4f,esnet_AUC_mean: %.4f '%(esnet_auc.mean(),esnet_auc.std() ) )\n",
    "print('rf_AUC_mean: %.4f,rf_AUC_std: %.4f '%(rf_auc.mean(),rf_auc.std() ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-3.35738211e+00,  3.29937659e+00,  1.35294667e-02,\n",
       "         2.18516424e-01,  2.48616333e-01, -2.21618577e+00,\n",
       "         1.97401538e+00, -1.71221473e-02, -1.22510143e-01,\n",
       "        -5.49657909e-02, -1.23690301e+00,  1.05391641e+00,\n",
       "         1.23759977e-01, -5.49860576e-02, -6.20363376e-02,\n",
       "        -7.38344665e-02, -1.66216076e-02, -1.53572276e-01,\n",
       "         2.00061718e-04,  2.14143419e-01,  1.39165756e-01,\n",
       "         1.02176841e-01,  1.81614700e-01,  9.35021195e-02,\n",
       "         1.18232271e-02, -1.31431475e-01, -2.28807181e-01,\n",
       "         3.93066093e-02,  7.75254502e-02, -2.19501529e-03,\n",
       "        -2.03904368e-01,  2.44134043e-01, -2.94695558e-01,\n",
       "         1.97691597e-01, -7.81276427e-02, -5.30709677e-02,\n",
       "         7.70586113e-03,  8.47172273e-02, -1.60885627e-01,\n",
       "        -3.49428285e-01,  1.48112764e-01,  1.03512076e-01,\n",
       "        -6.43640319e-02, -1.71003596e-01,  3.79105508e-01,\n",
       "        -1.87397671e-01, -1.38588743e-02,  1.35046949e-02,\n",
       "        -3.65503760e-01, -2.17860799e-02,  1.74588857e-01]])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "dict_lr['clf'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-3.90903706,  3.65560616, -0.        ,  0.        , -0.        ,\n",
       "       -2.64597942,  2.51789901, -0.        ,  0.01747499,  0.        ,\n",
       "       -1.29812849,  0.99660596,  0.        ,  0.42239975, -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.10566843])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "model.W_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([1, 1, 0, 0, 0]),\n",
       " array([1, 1, 0, 0, 0]),\n",
       " array([1, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0]),\n",
       " array([0, 1, 0, 1, 0]),\n",
       " array([1, 1, 0, 1, 0]),\n",
       " array([0, 1, 1, 1, 1]),\n",
       " array([0, 1, 0, 1, 1]),\n",
       " array([1, 0, 0, 0, 0]),\n",
       " array([1, 0, 1, 0, 1])]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model.s_mean"
   ]
  }
 ]
}